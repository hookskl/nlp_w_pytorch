{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_surname_generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzrfHz3rYMOWAWIfWWfI5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_surname_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGOxxG14NTiF"
      },
      "source": [
        "# Surname Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crCj2BA2OJqG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmsxoUZZOPe7"
      },
      "source": [
        "### Downloader and Surnames dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmJxXS1uOfp6"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "# get python file shell script\r\n",
        "curl -o download.py https://raw.githubusercontent.com/hookskl/PyTorchNLPBook/master/data/download.py\r\n",
        "#! /bin/bash\r\n",
        "\r\n",
        "# For each file, add a download.py line\r\n",
        "# Any additional processing on the downloaded file\r\n",
        "\r\n",
        "HERE=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\r\n",
        "\r\n",
        "# Surnames Dataset\r\n",
        "mkdir -p $HERE/surnames\r\n",
        "if [ ! -f $HERE/surnames/surnames_with_splits.csv ]; then\r\n",
        "    python download.py 1T1la2tYO1O7XkMRawG8VcFcvtjbxDqU- $HERE/surnames/surnames_with_splits.csv # 8\r\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5FyUSvINXO_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYUtR_xcNCKK"
      },
      "source": [
        "import os\r\n",
        "from argparse import Namespace\r\n",
        "from collections import Counter\r\n",
        "import json\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YVwSX92NcGd"
      },
      "source": [
        "## Data Vectorization Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iydlLsEYNfCV"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWFz_h_XNeZ-"
      },
      "source": [
        "class Vocabulary(object):\r\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, token_to_idx=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if token_to_idx is None:\r\n",
        "            token_to_idx = {}\r\n",
        "        self._token_to_idx = token_to_idx\r\n",
        "\r\n",
        "        self._idx_to_token = {idx: token\r\n",
        "                              for token, idx in self._token_to_idx.items()}\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        \"\"\" returns a dictionary that can be serialized\"\"\"\r\n",
        "        return {'token_to_idx': self._token_to_idx}\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_serializable(cls, contents):\r\n",
        "        \"\"\"instantiates the Vocabulary from a serialized dictionary\"\"\"\r\n",
        "        return cls(**contents)\r\n",
        "\r\n",
        "    def add_token(self, token):\r\n",
        "        \"\"\"Update mapping dicst based on the token.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            token (str): the item to add into the Vocabulary\r\n",
        "        Returns:\r\n",
        "            index (int): the integer corresponding to the token\r\n",
        "        \"\"\"\r\n",
        "        if token in self._token_to_idx:\r\n",
        "            index = self._token_to_idx[token]   \r\n",
        "\r\n",
        "        else:\r\n",
        "            index = len(self._token_to_idx)\r\n",
        "            self._token_to_idx[token] = index\r\n",
        "            self._idx_to_token[index] = token\r\n",
        "\r\n",
        "        return index\r\n",
        "\r\n",
        "    def add_many(self, tokens):\r\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\r\n",
        "\r\n",
        "        Args:\r\n",
        "            tokens (list): a list of string tokens\r\n",
        "        Returns:\r\n",
        "            indices (list): a list of indices corresponding to the tokens\r\n",
        "        \"\"\"\r\n",
        "        return [self.add_token(token) for token in tokens]\r\n",
        "\r\n",
        "    def lookup_token(self, token):\r\n",
        "        \"\"\"Retrieve the index associated with the token\r\n",
        "\r\n",
        "        Args: \r\n",
        "            token (str): the token to look up\r\n",
        "        Returns:\r\n",
        "            index (int): the index corresponding to the token\r\n",
        "        \"\"\"\r\n",
        "        return  self._token_to_idx[token]\r\n",
        "\r\n",
        "    def lookup_index(self, index):\r\n",
        "        \"\"\"Return the token associated with the index\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to look up\r\n",
        "        Returns:\r\n",
        "            token (str): the token corresponding to the index\r\n",
        "        Raises:\r\n",
        "            KeyError: if the index is not in the Vocabulary\r\n",
        "        \"\"\"\r\n",
        "        if index not in self._idx_to_token:\r\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\r\n",
        "        return self._idx_to_token[index]\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return (len(self._token_to_idx))\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLJPvhyVToU5"
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\r\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\r\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\r\n",
        "                 end_seq_token=\"<END>\"):\r\n",
        "      \r\n",
        "        super(SequenceVocabulary, self).__init_(token_to_idx)\r\n",
        "\r\n",
        "        self._mask_token = mask_token\r\n",
        "        self._unk_token = unk_token \r\n",
        "        self._begin_seq_token = begin_seq_token\r\n",
        "        self._end_seq_token = end_seq_token\r\n",
        "\r\n",
        "        self.mask_index = self.add_token(self._mask_token)\r\n",
        "        self.unk_index = self.add_token(self._unk_token)\r\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\r\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\r\n",
        "        contents.update({'unk_token': self._unk_token,\r\n",
        "                         'mask_token': self._mask_token,\r\n",
        "                         'begin_seq_token': self._begin_seq_token,\r\n",
        "                         'end_seq_token': self._end_seq_token})\r\n",
        "        return contents\r\n",
        "\r\n",
        "    def lookup_token(self, token):\r\n",
        "        \"\"\"Retrieve the index associated with the token\r\n",
        "        or the UNK index if token isn't present.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            token (str): the token to look up\r\n",
        "        Returns:\r\n",
        "            index (int): the index corresponding to the token\r\n",
        "        Notes:\r\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\r\n",
        "              for the UNK functionality\r\n",
        "        \"\"\"\r\n",
        "        if self.unk_index >= 0:\r\n",
        "            return self._token_to_idx.get(token, self.unk_index)\r\n",
        "        else:\r\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0efrmuc9NhhB"
      },
      "source": [
        "### Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDDuVF0uNicJ"
      },
      "source": [
        "class SurnameVectorizer(object):\r\n",
        "    \"\"\"The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\r\n",
        "    def __init__(self, char_vocab, nationality_vocab):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            char_vocab (SequenceVocabulary): maps surname characters to integers\r\n",
        "            nationality_vocab (Vocabulary): maps nationalitities to integers\r\n",
        "        \"\"\"\r\n",
        "        self.char_vocab = char_vocab\r\n",
        "        self.nationality_vocab = nationality_vocab\r\n",
        "\r\n",
        "    def vectorizer(self, surname, vector_length=-1):\r\n",
        "        \"\"\"Vectorizer a surname into a vector of observations and targets\r\n",
        "\r\n",
        "        The outputs are the vectorized surname split into two vectors:\r\n",
        "            surname[:-1] and surname[1:]\r\n",
        "        At each timestep, the first vector is the observation and the second vector is the target.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname (str): the surname to be vectorized\r\n",
        "            vector_length (int): an argument for forcing the length of the index vector\r\n",
        "        Returns:\r\n",
        "            a tuple: (from_vector, to_vector)\r\n",
        "            from_vector (numpy.ndarray): the observation vector\r\n",
        "            to_vector (numpy.ndarray): the target prediction vector\r\n",
        "        \"\"\"\r\n",
        "        indices = [self.char_vocab.begin_seq_index]\r\n",
        "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\r\n",
        "        indices.append(self.char_vocab.end_seq_index)\r\n",
        "\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices) - 1\r\n",
        "\r\n",
        "        from_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        from_indices = indices[:-1]\r\n",
        "        from_vector[:len(from_indices)] = from_indices\r\n",
        "        from_vector[len(from_indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        to_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        to_indices = indices[1:]\r\n",
        "        to_vector[:len(to_indices)] = to_indices \r\n",
        "        to_vector[len(to_indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        return from_vector, to_vector \r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_dataframe(cls, surname_df):\r\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_df (pandas.DataFrame): the surname dataset\r\n",
        "        Returns:\r\n",
        "            an instance of the SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary()\r\n",
        "        nationality_vocab = Vocabulary()\r\n",
        "\r\n",
        "        for index, row in surname_df.iterrows():\r\n",
        "            for char in row.surname:\r\n",
        "                char_vocab.add_token(char)\r\n",
        "            nationality_vocab.add_token(row.nationality) \r\n",
        "\r\n",
        "        return cls(char_vocab, nationality_vocab)\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_serializable(cls, contents):\r\n",
        "        \"\"\"Instantiate the vectorizer from saved contents\r\n",
        "\r\n",
        "        Args:\r\n",
        "            contents (dict): a dict holding two vocabularies for the vectorizer\r\n",
        "                this dictionary is created using `vectorizer.to_serializable()`\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\r\n",
        "        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\r\n",
        "\r\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        \"\"\"Returns the serializable contents\"\"\"\r\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(),\r\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwK1f6l1Njd-"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcyk3N7iNkUM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUg_07YINoBV"
      },
      "source": [
        "## The Model: SurnameGenerationModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QJIIDKtNq2A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy-2hSXkNvCC"
      },
      "source": [
        "### Training Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aTaTzBDNxQh"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYfV9A0gNwvz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODuBnmlN0jq"
      },
      "source": [
        "#### General Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLMs0UdVN240"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R32aKZ9YN4Ba"
      },
      "source": [
        "#### Settings and Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGbO2zgN5ku"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrlYbaewN6kF"
      },
      "source": [
        "#### Initializations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChtIGSn8N8Jx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaIKDbWLN8yo"
      },
      "source": [
        "#### Training Loop\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2arODRxN-IY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmDRzksNOAnJ"
      },
      "source": [
        "### Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQlCW4EZOC6q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdWq_ecMODxh"
      },
      "source": [
        "### Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8JnrUSqOF1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}