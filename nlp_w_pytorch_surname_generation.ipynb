{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp_w_pytorch_surname_generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMe2iKl3Jq/8ah8fMttvx4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d39ec6ca8624388854bff979147ccc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c0bbbef1e874e1f8ffe913730312b90",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13f11f93d89549a795bd2f4d17e84959",
              "IPY_MODEL_78566f52c6114dbba75915df0f89c31b"
            ]
          }
        },
        "6c0bbbef1e874e1f8ffe913730312b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13f11f93d89549a795bd2f4d17e84959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_691cca946b3d483687f71531f714a393",
            "_dom_classes": [],
            "description": "training routine: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_234e892d0c644bceb03a25dc478e206c"
          }
        },
        "78566f52c6114dbba75915df0f89c31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fda6bbc1822c440abd07a88520913706",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 200/200 [08:57&lt;00:00,  2.65s/it, sample1=German-&gt;Papwer, sample2=Spanish-&gt;Korzinra]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9deec7c3cd94fbb9293f61d985f8e23"
          }
        },
        "691cca946b3d483687f71531f714a393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "234e892d0c644bceb03a25dc478e206c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fda6bbc1822c440abd07a88520913706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9deec7c3cd94fbb9293f61d985f8e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afcdcbee127e4ef593de336da8feeb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cec70595ec454a99846f37f6f1ecf863",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c12c00a404d4cb9898c3c02404839a4",
              "IPY_MODEL_310318ff05c4450e91c4467488ad8ca2"
            ]
          }
        },
        "cec70595ec454a99846f37f6f1ecf863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c12c00a404d4cb9898c3c02404839a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1651248ea06644ae9564d787922a9761",
            "_dom_classes": [],
            "description": "split=train:  98%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 60,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 59,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_506c5d4e7c4040cb8c67dc6ee1d61d9b"
          }
        },
        "310318ff05c4450e91c4467488ad8ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29473c918dc849e386cb1e4e50049a83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 59/60 [08:57&lt;00:04,  4.86s/it, acc=37.8, epoch=199, loss=2.03]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_066792a724764991b5e8d5cf43aa3f81"
          }
        },
        "1651248ea06644ae9564d787922a9761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "506c5d4e7c4040cb8c67dc6ee1d61d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29473c918dc849e386cb1e4e50049a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "066792a724764991b5e8d5cf43aa3f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9ad0beb0a4f4663a2a76626ec0f70de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_024cb88200804801987cb67d25548155",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2605c229600c4a5abb70e345238b1fba",
              "IPY_MODEL_0e5696174208407ebc93c842b14942a2"
            ]
          }
        },
        "024cb88200804801987cb67d25548155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2605c229600c4a5abb70e345238b1fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7140336591244fa8e6a418c1a85194a",
            "_dom_classes": [],
            "description": "split=val:  92%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 12,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 11,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f12614a2eefb4d1bac91ecba556ff196"
          }
        },
        "0e5696174208407ebc93c842b14942a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5ffe91f22ea45419084ba478f9d2769",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11/12 [08:57&lt;00:01,  1.09s/it, acc=35.7, epoch=199, loss=2.14]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fbf3ea7da7e4950b182aeb748c74c8e"
          }
        },
        "c7140336591244fa8e6a418c1a85194a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f12614a2eefb4d1bac91ecba556ff196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5ffe91f22ea45419084ba478f9d2769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fbf3ea7da7e4950b182aeb748c74c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_surname_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGOxxG14NTiF"
      },
      "source": [
        "# Surname Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crCj2BA2OJqG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmsxoUZZOPe7"
      },
      "source": [
        "### Downloader and Surnames dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmJxXS1uOfp6",
        "outputId": "ce42c0cd-8ecc-4e38-c1c9-4d4cdad17ce4"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "# get python file shell script\r\n",
        "curl -o download.py https://raw.githubusercontent.com/hookskl/PyTorchNLPBook/master/data/download.py\r\n",
        "#! /bin/bash\r\n",
        "\r\n",
        "# For each file, add a download.py line\r\n",
        "# Any additional processing on the downloaded file\r\n",
        "\r\n",
        "HERE=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\r\n",
        "\r\n",
        "# Surnames Dataset\r\n",
        "mkdir -p $HERE/data/surnames\r\n",
        "if [ ! -f $HERE/surnames/surnames_with_splits.csv ]; then\r\n",
        "    python download.py 1T1la2tYO1O7XkMRawG8VcFcvtjbxDqU- $HERE/data/surnames/surnames_with_splits.csv # 8\r\n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1572  100  1572    0     0   4271      0 --:--:-- --:--:-- --:--:--  4271\n",
            "Trying to fetch /content/data/surnames/surnames_with_splits.csv\n",
            "8it [00:00, 2771.03it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5FyUSvINXO_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYUtR_xcNCKK"
      },
      "source": [
        "import os\r\n",
        "from argparse import Namespace\r\n",
        "from collections import Counter\r\n",
        "import json\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YVwSX92NcGd"
      },
      "source": [
        "## Data Vectorization Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iydlLsEYNfCV"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWFz_h_XNeZ-"
      },
      "source": [
        "class Vocabulary(object):\r\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, token_to_idx=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if token_to_idx is None:\r\n",
        "            token_to_idx = {}\r\n",
        "        self._token_to_idx = token_to_idx\r\n",
        "\r\n",
        "        self._idx_to_token = {idx: token\r\n",
        "                              for token, idx in self._token_to_idx.items()}\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        \"\"\" returns a dictionary that can be serialized\"\"\"\r\n",
        "        return {'token_to_idx': self._token_to_idx}\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_serializable(cls, contents):\r\n",
        "        \"\"\"instantiates the Vocabulary from a serialized dictionary\"\"\"\r\n",
        "        return cls(**contents)\r\n",
        "\r\n",
        "    def add_token(self, token):\r\n",
        "        \"\"\"Update mapping dicst based on the token.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            token (str): the item to add into the Vocabulary\r\n",
        "        Returns:\r\n",
        "            index (int): the integer corresponding to the token\r\n",
        "        \"\"\"\r\n",
        "        if token in self._token_to_idx:\r\n",
        "            index = self._token_to_idx[token]   \r\n",
        "\r\n",
        "        else:\r\n",
        "            index = len(self._token_to_idx)\r\n",
        "            self._token_to_idx[token] = index\r\n",
        "            self._idx_to_token[index] = token\r\n",
        "\r\n",
        "        return index\r\n",
        "\r\n",
        "    def add_many(self, tokens):\r\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\r\n",
        "\r\n",
        "        Args:\r\n",
        "            tokens (list): a list of string tokens\r\n",
        "        Returns:\r\n",
        "            indices (list): a list of indices corresponding to the tokens\r\n",
        "        \"\"\"\r\n",
        "        return [self.add_token(token) for token in tokens]\r\n",
        "\r\n",
        "    def lookup_token(self, token):\r\n",
        "        \"\"\"Retrieve the index associated with the token\r\n",
        "\r\n",
        "        Args: \r\n",
        "            token (str): the token to look up\r\n",
        "        Returns:\r\n",
        "            index (int): the index corresponding to the token\r\n",
        "        \"\"\"\r\n",
        "        return  self._token_to_idx[token]\r\n",
        "\r\n",
        "    def lookup_index(self, index):\r\n",
        "        \"\"\"Return the token associated with the index\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to look up\r\n",
        "        Returns:\r\n",
        "            token (str): the token corresponding to the index\r\n",
        "        Raises:\r\n",
        "            KeyError: if the index is not in the Vocabulary\r\n",
        "        \"\"\"\r\n",
        "        if index not in self._idx_to_token:\r\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\r\n",
        "        return self._idx_to_token[index]\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return (len(self._token_to_idx))\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLJPvhyVToU5"
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\r\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\r\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\r\n",
        "                 end_seq_token=\"<END>\"):\r\n",
        "\r\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\r\n",
        "\r\n",
        "        self._mask_token = mask_token\r\n",
        "        self._unk_token = unk_token\r\n",
        "        self._begin_seq_token = begin_seq_token\r\n",
        "        self._end_seq_token = end_seq_token\r\n",
        "\r\n",
        "        self.mask_index = self.add_token(self._mask_token)\r\n",
        "        self.unk_index = self.add_token(self._unk_token)\r\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\r\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\r\n",
        "        contents.update({'unk_token': self._unk_token,\r\n",
        "                         'mask_token': self._mask_token,\r\n",
        "                         'begin_seq_token': self._begin_seq_token,\r\n",
        "                         'end_seq_token': self._end_seq_token})\r\n",
        "        return contents\r\n",
        "\r\n",
        "    def lookup_token(self, token):\r\n",
        "        \"\"\"Retrieve the index associated with the token \r\n",
        "          or the UNK index if token isn't present.\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            token (str): the token to look up \r\n",
        "        Returns:\r\n",
        "            index (int): the index corresponding to the token\r\n",
        "        Notes:\r\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \r\n",
        "              for the UNK functionality \r\n",
        "        \"\"\"\r\n",
        "        if self.unk_index >= 0:\r\n",
        "            return self._token_to_idx.get(token, self.unk_index)\r\n",
        "        else:\r\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0efrmuc9NhhB"
      },
      "source": [
        "### Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDDuVF0uNicJ"
      },
      "source": [
        "class SurnameVectorizer(object):\r\n",
        "    \"\"\"The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\r\n",
        "    def __init__(self, char_vocab, nationality_vocab):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            char_vocab (SequenceVocabulary): maps surname characters to integers\r\n",
        "            nationality_vocab (Vocabulary): maps nationalitities to integers\r\n",
        "        \"\"\"\r\n",
        "        self.char_vocab = char_vocab\r\n",
        "        self.nationality_vocab = nationality_vocab\r\n",
        "\r\n",
        "    def vectorize(self, surname, vector_length=-1):\r\n",
        "        \"\"\"Vectorizer a surname into a vector of observations and targets\r\n",
        "\r\n",
        "        The outputs are the vectorized surname split into two vectors:\r\n",
        "            surname[:-1] and surname[1:]\r\n",
        "        At each timestep, the first vector is the observation and the second vector is the target.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname (str): the surname to be vectorized\r\n",
        "            vector_length (int): an argument for forcing the length of the index vector\r\n",
        "        Returns:\r\n",
        "            a tuple: (from_vector, to_vector)\r\n",
        "            from_vector (numpy.ndarray): the observation vector\r\n",
        "            to_vector (numpy.ndarray): the target prediction vector\r\n",
        "        \"\"\"\r\n",
        "        indices = [self.char_vocab.begin_seq_index]\r\n",
        "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\r\n",
        "        indices.append(self.char_vocab.end_seq_index)\r\n",
        "\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices) - 1\r\n",
        "\r\n",
        "        from_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        from_indices = indices[:-1]\r\n",
        "        from_vector[:len(from_indices)] = from_indices\r\n",
        "        from_vector[len(from_indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        to_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        to_indices = indices[1:]\r\n",
        "        to_vector[:len(to_indices)] = to_indices \r\n",
        "        to_vector[len(to_indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        return from_vector, to_vector \r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_dataframe(cls, surname_df):\r\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_df (pandas.DataFrame): the surname dataset\r\n",
        "        Returns:\r\n",
        "            an instance of the SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary()\r\n",
        "        nationality_vocab = Vocabulary()\r\n",
        "\r\n",
        "        for index, row in surname_df.iterrows():\r\n",
        "            for char in row.surname:\r\n",
        "                char_vocab.add_token(char)\r\n",
        "            nationality_vocab.add_token(row.nationality) \r\n",
        "\r\n",
        "        return cls(char_vocab, nationality_vocab)\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_serializable(cls, contents):\r\n",
        "        \"\"\"Instantiate the vectorizer from saved contents\r\n",
        "\r\n",
        "        Args:\r\n",
        "            contents (dict): a dict holding two vocabularies for the vectorizer\r\n",
        "                this dictionary is created using `vectorizer.to_serializable()`\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\r\n",
        "        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\r\n",
        "\r\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\r\n",
        "\r\n",
        "    def to_serializable(self):\r\n",
        "        \"\"\"Returns the serializable contents\"\"\"\r\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(),\r\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwK1f6l1Njd-"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcyk3N7iNkUM"
      },
      "source": [
        "class SurnameDataset(Dataset):\r\n",
        "    def __init__(self, surname_df, vectorizer):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            surname_df (pandas.DataFrame): the dataset\r\n",
        "            vectorizer (SurnameVectorizer): vectorizer instantiated from dataset\r\n",
        "        \"\"\"\r\n",
        "        self.surname_df = surname_df\r\n",
        "        self._vectorizer = vectorizer \r\n",
        "\r\n",
        "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2 # plus 2 for the beg/end tokens\r\n",
        "\r\n",
        "        self.train_df = self.surname_df[self.surname_df.split=='train']\r\n",
        "        self.train_size = len(self.train_df)\r\n",
        "\r\n",
        "        self.val_df = self.surname_df[self.surname_df.split=='val']\r\n",
        "        self.validation_size = len(self.val_df)\r\n",
        "\r\n",
        "        self.test_df = self.surname_df[self.surname_df.split=='test']\r\n",
        "        self.test_size = len(self.test_df)\r\n",
        "\r\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size), \r\n",
        "                             'val': (self.val_df, self.validation_size), \r\n",
        "                             'test': (self.test_df, self.test_size)}\r\n",
        "\r\n",
        "        self.set_split('train')\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\r\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_csv (str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameDataset\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        surname_df = pd.read_csv(surname_csv)\r\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(surname_df))\r\n",
        "        \r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\r\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \r\n",
        "        Used in the case in the vectorizer has been cached for re-use\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            surname_csv (str): location of the dataset\r\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameDataset\r\n",
        "        \"\"\"\r\n",
        "        surname_df = pd.read_csv(surname_csv)\r\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\r\n",
        "        return cls(surname_df, vectorizer)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def load_vectorizer_only(vectorizer_filepath):\r\n",
        "        \"\"\"a static method for loading the vectorizer from file\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        with open(vectorizer_filepath) as fp:\r\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\r\n",
        "\r\n",
        "    def save_vectorizer(self, vectorizer_filepath):\r\n",
        "        \"\"\"saves the vectorizer to disk using json\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\r\n",
        "        \"\"\"\r\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\r\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\r\n",
        "\r\n",
        "    def get_vectorizer(self):\r\n",
        "        \"\"\" returns the vectorizer \"\"\"\r\n",
        "        return self._vectorizer\r\n",
        "\r\n",
        "    def set_split(self, split=\"train\"):\r\n",
        "        self._target_split = split\r\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self._target_size\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point \r\n",
        "        Returns:\r\n",
        "            a dictionary holding the data point: (x_data, y_target, class_index)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "        \r\n",
        "        from_vector, to_vector = \\\r\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\r\n",
        "        \r\n",
        "        nationality_index = \\\r\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\r\n",
        "\r\n",
        "        return {'x_data': from_vector, \r\n",
        "                'y_target': to_vector, \r\n",
        "                'class_index': nationality_index}\r\n",
        "\r\n",
        "    def get_num_batches(self, batch_size):\r\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            batch_size (int)\r\n",
        "        Returns:\r\n",
        "            number of batches in the dataset\r\n",
        "        \"\"\"\r\n",
        "        return len(self) // batch_size\r\n",
        "    \r\n",
        "def generate_batches(dataset, batch_size, shuffle=True,\r\n",
        "                     drop_last=True, device=\"cpu\"): \r\n",
        "    \"\"\"\r\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \r\n",
        "      ensure each tensor is on the write device location.\r\n",
        "    \"\"\"\r\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\r\n",
        "                            shuffle=shuffle, drop_last=drop_last)\r\n",
        "\r\n",
        "    for data_dict in dataloader:\r\n",
        "        out_data_dict = {}\r\n",
        "        for name, tensor in data_dict.items():\r\n",
        "            out_data_dict[name] = data_dict[name].to(device)\r\n",
        "        yield out_data_dict        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUg_07YINoBV"
      },
      "source": [
        "## The Model: SurnameGenerationModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QJIIDKtNq2A"
      },
      "source": [
        "class SurnameGenerationModel(nn.Module):\r\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\r\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            char_embedding_size (int): The size of the character embeddings\r\n",
        "            char_vocab_size (int): The number of characters to embed\r\n",
        "            num_nationalities (int): The size of the prediction vector \r\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\r\n",
        "            batch_first (bool): Informs whether the input tensors will \r\n",
        "                have batch or the sequence on the 0th dimension\r\n",
        "            padding_idx (int): The index for the tensor padding; \r\n",
        "                see torch.nn.Embedding\r\n",
        "            dropout_p (float): the probability of zeroing activations using\r\n",
        "                the dropout method.  higher means more likely to zero.\r\n",
        "        \"\"\"\r\n",
        "        super(SurnameGenerationModel, self).__init__()\r\n",
        "        \r\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\r\n",
        "                                     embedding_dim=char_embedding_size,\r\n",
        "                                     padding_idx=padding_idx)\r\n",
        "\r\n",
        "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\r\n",
        "                                       embedding_dim=rnn_hidden_size)\r\n",
        "\r\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size, \r\n",
        "                          hidden_size=rnn_hidden_size,\r\n",
        "                          batch_first=batch_first)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size, \r\n",
        "                            out_features=char_vocab_size)\r\n",
        "        \r\n",
        "        self._dropout_p = dropout_p\r\n",
        "\r\n",
        "    def forward(self, x_in, nationality_index, apply_softmax=False):\r\n",
        "        \"\"\"The forward pass of the model\r\n",
        "        \r\n",
        "        Args:\r\n",
        "            x_in (torch.Tensor): an input data tensor. \r\n",
        "                x_in.shape should be (batch, max_seq_size)\r\n",
        "            nationality_index (torch.Tensor): The index of the nationality for each data point\r\n",
        "                Used to initialize the hidden state of the RNN\r\n",
        "            apply_softmax (bool): a flag for the softmax activation\r\n",
        "                should be false if used with the Cross Entropy losses\r\n",
        "        Returns:\r\n",
        "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\r\n",
        "        \"\"\"\r\n",
        "        x_embedded = self.char_emb(x_in)\r\n",
        "        \r\n",
        "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\r\n",
        "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\r\n",
        "\r\n",
        "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\r\n",
        "\r\n",
        "        batch_size, seq_size, feat_size = y_out.shape\r\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\r\n",
        "\r\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\r\n",
        "                         \r\n",
        "        if apply_softmax:\r\n",
        "            y_out = F.softmax(y_out, dim=1)\r\n",
        "\r\n",
        "        new_feat_size = y_out.shape[-1]\r\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\r\n",
        "            \r\n",
        "        return y_out "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMy6pi5-k5wB"
      },
      "source": [
        "def sample_from_model(model, vectorizer, nationalities, sample_size=20, \r\n",
        "                      temperature=1.0):\r\n",
        "    \"\"\"Sample a sequence of indices from the model\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        model (SurnameGenerationModel): the trained model\r\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\r\n",
        "        nationalities (list): a list of integers representing nationalities\r\n",
        "        sample_size (int): the max length of the samples\r\n",
        "        temperature (float): accentuates or flattens \r\n",
        "            the distribution. \r\n",
        "            0.0 < temperature < 1.0 will make it peakier. \r\n",
        "            temperature > 1.0 will make it more uniform\r\n",
        "    Returns:\r\n",
        "        indices (torch.Tensor): the matrix of indices; \r\n",
        "        shape = (num_samples, sample_size)\r\n",
        "    \"\"\"\r\n",
        "    num_samples = len(nationalities)\r\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \r\n",
        "                       for _ in range(num_samples)]\r\n",
        "    begin_seq_index = torch.tensor(begin_seq_index, \r\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\r\n",
        "    indices = [begin_seq_index]\r\n",
        "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\r\n",
        "    h_t = model.nation_emb(nationality_indices)\r\n",
        "    \r\n",
        "    for time_step in range(sample_size):\r\n",
        "        x_t = indices[time_step]\r\n",
        "        x_emb_t = model.char_emb(x_t)\r\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\r\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\r\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\r\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\r\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\r\n",
        "    return indices\r\n",
        "\r\n",
        "def decode_samples(sampled_indices, vectorizer):\r\n",
        "    \"\"\"Transform indices into the string form of a surname\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\r\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\r\n",
        "    \"\"\"\r\n",
        "    decoded_surnames = []\r\n",
        "    vocab = vectorizer.char_vocab\r\n",
        "    \r\n",
        "    for sample_index in range(sampled_indices.shape[0]):\r\n",
        "        surname = \"\"\r\n",
        "        for time_step in range(sampled_indices.shape[1]):\r\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\r\n",
        "            if sample_item == vocab.begin_seq_index:\r\n",
        "                continue\r\n",
        "            elif sample_item == vocab.end_seq_index:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                surname += vocab.lookup_index(sample_item)\r\n",
        "        decoded_surnames.append(surname)\r\n",
        "    return decoded_surnames"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy-2hSXkNvCC"
      },
      "source": [
        "### Training Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aTaTzBDNxQh"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYfV9A0gNwvz"
      },
      "source": [
        "def make_train_state(args):\r\n",
        "    return {'stop_early': False,\r\n",
        "            'early_stopping_step': 0,\r\n",
        "            'early_stopping_best_val': 1e8,\r\n",
        "            'learning_rate': args.learning_rate,\r\n",
        "            'epoch_index': 0,\r\n",
        "            'train_loss': [],\r\n",
        "            'train_acc': [],\r\n",
        "            'val_loss': [],\r\n",
        "            'val_acc': [],\r\n",
        "            'test_loss': -1,\r\n",
        "            'test_acc': -1,\r\n",
        "            'model_filename': args.model_state_file}\r\n",
        "\r\n",
        "def update_train_state(args, model, train_state):\r\n",
        "    \"\"\"Handle the training state updates.\r\n",
        "    Components:\r\n",
        "     - Early Stopping: Prevent overfitting.\r\n",
        "     - Model Checkpoint: Model is saved if the model is better\r\n",
        "    \r\n",
        "    :param args: main arguments\r\n",
        "    :param model: model to train\r\n",
        "    :param train_state: a dictionary representing the training state values\r\n",
        "    :returns:\r\n",
        "        a new train_state\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Save one model at least\r\n",
        "    if train_state['epoch_index'] == 0:\r\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\r\n",
        "        train_state['stop_early'] = False\r\n",
        "\r\n",
        "    # Save model if performance improved\r\n",
        "    elif train_state['epoch_index'] >= 1:\r\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\r\n",
        "         \r\n",
        "        # If loss worsened\r\n",
        "        if loss_t >= loss_tm1:\r\n",
        "            # Update step\r\n",
        "            train_state['early_stopping_step'] += 1\r\n",
        "        # Loss decreased\r\n",
        "        else:\r\n",
        "            # Save the best model\r\n",
        "            if loss_t < train_state['early_stopping_best_val']:\r\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\r\n",
        "                train_state['early_stopping_best_val'] = loss_t\r\n",
        "\r\n",
        "            # Reset early stopping step\r\n",
        "            train_state['early_stopping_step'] = 0\r\n",
        "\r\n",
        "        # Stop early ?\r\n",
        "        train_state['stop_early'] = \\\r\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\r\n",
        "\r\n",
        "    return train_state\r\n",
        "\r\n",
        "def normalize_sizes(y_pred, y_true):\r\n",
        "    \"\"\"Normalize tensor sizes\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        y_pred (torch.Tensor): the output of the model\r\n",
        "            If a 3-dimensional tensor, reshapes to a matrix\r\n",
        "        y_true (torch.Tensor): the target predictions\r\n",
        "            If a matrix, reshapes to be a vector\r\n",
        "    \"\"\"\r\n",
        "    if len(y_pred.size()) == 3:\r\n",
        "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\r\n",
        "    if len(y_true.size()) == 2:\r\n",
        "        y_true = y_true.contiguous().view(-1)\r\n",
        "    return y_pred, y_true\r\n",
        "\r\n",
        "def compute_accuracy(y_pred, y_true, mask_index):\r\n",
        "    y_pred, y_true = normalize_sizes(y_pred, y_true)\r\n",
        "\r\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\r\n",
        "    \r\n",
        "    correct_indices = torch.eq(y_pred_indices, y_true).float()\r\n",
        "    valid_indices = torch.ne(y_true, mask_index).float()\r\n",
        "    \r\n",
        "    n_correct = (correct_indices * valid_indices).sum().item()\r\n",
        "    n_valid = valid_indices.sum().item()\r\n",
        "\r\n",
        "    return n_correct / n_valid * 100\r\n",
        "\r\n",
        "def sequence_loss(y_pred, y_true, mask_index):\r\n",
        "    y_pred, y_true = normalize_sizes(y_pred, y_true)\r\n",
        "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODuBnmlN0jq"
      },
      "source": [
        "#### General Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLMs0UdVN240"
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    if cuda:\r\n",
        "        torch.cuda.manual_seed_all(seed)\r\n",
        "\r\n",
        "def handle_dirs(dirpath):\r\n",
        "    if not os.path.exists(dirpath):\r\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R32aKZ9YN4Ba"
      },
      "source": [
        "#### Settings and Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SGbO2zgN5ku",
        "outputId": "1daeb8f4-9154-4ade-cd43-c8356672e60d"
      },
      "source": [
        "args = Namespace(\r\n",
        "    # Data and Path information\r\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\r\n",
        "    vectorizer_file=\"vectorizer.json\",\r\n",
        "    model_state_file=\"model.pth\",\r\n",
        "    save_dir=\"model_storage/ch7/model2_conditioned_surname_generation\",\r\n",
        "    # Model hyper parameters\r\n",
        "    char_embedding_size=32,\r\n",
        "    rnn_hidden_size=128,\r\n",
        "    # Training hyper parameters\r\n",
        "    seed=1337,\r\n",
        "    learning_rate=0.001,\r\n",
        "    batch_size=128,\r\n",
        "    num_epochs=200,\r\n",
        "    early_stopping_criteria=5,\r\n",
        "    # Runtime options\r\n",
        "    catch_keyboard_interrupt=True,\r\n",
        "    cuda=True,\r\n",
        "    expand_filepaths_to_save_dir=True,\r\n",
        "    reload_from_files=False,\r\n",
        ")\r\n",
        "\r\n",
        "if args.expand_filepaths_to_save_dir:\r\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\r\n",
        "                                        args.vectorizer_file)\r\n",
        "\r\n",
        "    args.model_state_file = os.path.join(args.save_dir,\r\n",
        "                                         args.model_state_file)\r\n",
        "    \r\n",
        "    print(\"Expanded filepaths: \")\r\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\r\n",
        "    print(\"\\t{}\".format(args.model_state_file))\r\n",
        "    \r\n",
        "# Check CUDA\r\n",
        "if not torch.cuda.is_available():\r\n",
        "    args.cuda = False\r\n",
        "\r\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\r\n",
        "    \r\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\r\n",
        "\r\n",
        "# Set seed for reproducibility\r\n",
        "set_seed_everywhere(args.seed, args.cuda)\r\n",
        "\r\n",
        "# handle dirs\r\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tmodel_storage/ch7/model2_conditioned_surname_generation/vectorizer.json\n",
            "\tmodel_storage/ch7/model2_conditioned_surname_generation/model.pth\n",
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrlYbaewN6kF"
      },
      "source": [
        "#### Initializations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChtIGSn8N8Jx"
      },
      "source": [
        "if args.reload_from_files:\r\n",
        "    # training from a checkpoint\r\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\r\n",
        "                                                              args.vectorizer_file)\r\n",
        "else:\r\n",
        "    # create dataset and vectorizer\r\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\r\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\r\n",
        "\r\n",
        "vectorizer = dataset.get_vectorizer()\r\n",
        "\r\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\r\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\r\n",
        "                               num_nationalities=len(vectorizer.nationality_vocab),\r\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\r\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index,\r\n",
        "                               dropout_p=0.5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaIKDbWLN8yo"
      },
      "source": [
        "#### Training Loop\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "2d39ec6ca8624388854bff979147ccc7",
            "6c0bbbef1e874e1f8ffe913730312b90",
            "13f11f93d89549a795bd2f4d17e84959",
            "78566f52c6114dbba75915df0f89c31b",
            "691cca946b3d483687f71531f714a393",
            "234e892d0c644bceb03a25dc478e206c",
            "fda6bbc1822c440abd07a88520913706",
            "b9deec7c3cd94fbb9293f61d985f8e23",
            "afcdcbee127e4ef593de336da8feeb01",
            "cec70595ec454a99846f37f6f1ecf863",
            "9c12c00a404d4cb9898c3c02404839a4",
            "310318ff05c4450e91c4467488ad8ca2",
            "1651248ea06644ae9564d787922a9761",
            "506c5d4e7c4040cb8c67dc6ee1d61d9b",
            "29473c918dc849e386cb1e4e50049a83",
            "066792a724764991b5e8d5cf43aa3f81",
            "c9ad0beb0a4f4663a2a76626ec0f70de",
            "024cb88200804801987cb67d25548155",
            "2605c229600c4a5abb70e345238b1fba",
            "0e5696174208407ebc93c842b14942a2",
            "c7140336591244fa8e6a418c1a85194a",
            "f12614a2eefb4d1bac91ecba556ff196",
            "a5ffe91f22ea45419084ba478f9d2769",
            "8fbf3ea7da7e4950b182aeb748c74c8e"
          ]
        },
        "id": "E2arODRxN-IY",
        "outputId": "48f7c93c-e531-44c8-ea61-b377b882a434"
      },
      "source": [
        "mask_index = vectorizer.char_vocab.mask_index\r\n",
        "\r\n",
        "model = model.to(args.device)\r\n",
        "\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\r\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\r\n",
        "                                           mode='min', factor=0.5,\r\n",
        "                                           patience=1)\r\n",
        "train_state = make_train_state(args)\r\n",
        "\r\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \r\n",
        "                          total=args.num_epochs,\r\n",
        "                          position=0)\r\n",
        "\r\n",
        "dataset.set_split('train')\r\n",
        "train_bar = tqdm_notebook(desc='split=train',\r\n",
        "                          total=dataset.get_num_batches(args.batch_size), \r\n",
        "                          position=1, \r\n",
        "                          leave=True)\r\n",
        "dataset.set_split('val')\r\n",
        "val_bar = tqdm_notebook(desc='split=val',\r\n",
        "                        total=dataset.get_num_batches(args.batch_size), \r\n",
        "                        position=1, \r\n",
        "                        leave=True)\r\n",
        "\r\n",
        "try:\r\n",
        "    for epoch_index in range(args.num_epochs):\r\n",
        "        train_state['epoch_index'] = epoch_index\r\n",
        "\r\n",
        "        # Iterate over training dataset\r\n",
        "\r\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\r\n",
        "        dataset.set_split('train')\r\n",
        "        batch_generator = generate_batches(dataset, \r\n",
        "                                           batch_size=args.batch_size, \r\n",
        "                                           device=args.device)\r\n",
        "        running_loss = 0.0\r\n",
        "        running_acc = 0.0\r\n",
        "        model.train()\r\n",
        "        \r\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\r\n",
        "            # the training routine is these 5 steps:\r\n",
        "\r\n",
        "            # --------------------------------------    \r\n",
        "            # step 1. zero the gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            # step 2. compute the output\r\n",
        "            y_pred = model(x_in=batch_dict['x_data'], \r\n",
        "                           nationality_index=batch_dict['class_index'])\r\n",
        "\r\n",
        "            # step 3. compute the loss\r\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\r\n",
        "\r\n",
        "\r\n",
        "            # step 4. use loss to produce gradients\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            # step 5. use optimizer to take gradient step\r\n",
        "            optimizer.step()\r\n",
        "            # -----------------------------------------\r\n",
        "            # compute the  running loss and running accuracy\r\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\r\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\r\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\r\n",
        "\r\n",
        "            # update bar\r\n",
        "            train_bar.set_postfix(loss=running_loss,\r\n",
        "                                  acc=running_acc,\r\n",
        "                                  epoch=epoch_index)\r\n",
        "            train_bar.update()\r\n",
        "\r\n",
        "        train_state['train_loss'].append(running_loss)\r\n",
        "        train_state['train_acc'].append(running_acc)\r\n",
        "\r\n",
        "        # Iterate over val dataset\r\n",
        "\r\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\r\n",
        "        dataset.set_split('val')\r\n",
        "        batch_generator = generate_batches(dataset, \r\n",
        "                                           batch_size=args.batch_size, \r\n",
        "                                           device=args.device)\r\n",
        "        running_loss = 0.\r\n",
        "        running_acc = 0.\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\r\n",
        "            # compute the output\r\n",
        "            y_pred = model(x_in=batch_dict['x_data'], \r\n",
        "                           nationality_index=batch_dict['class_index'])\r\n",
        "\r\n",
        "            # step 3. compute the loss\r\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\r\n",
        "\r\n",
        "            # compute the  running loss and running accuracy\r\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\r\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\r\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\r\n",
        "            \r\n",
        "            # Update bar\r\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \r\n",
        "                            epoch=epoch_index)\r\n",
        "            val_bar.update()\r\n",
        "\r\n",
        "        train_state['val_loss'].append(running_loss)\r\n",
        "        train_state['val_acc'].append(running_acc)\r\n",
        "\r\n",
        "        train_state = update_train_state(args=args, model=model, \r\n",
        "                                         train_state=train_state)\r\n",
        "\r\n",
        "        scheduler.step(train_state['val_loss'][-1])\r\n",
        "\r\n",
        "        if train_state['stop_early']:\r\n",
        "            break\r\n",
        "            \r\n",
        "        # move model to cpu for sampling\r\n",
        "        \r\n",
        "        nationalities = np.random.choice(np.arange(len(vectorizer.nationality_vocab)), replace=True, size=2)\r\n",
        "        model = model.cpu()\r\n",
        "        sampled_surnames = decode_samples(\r\n",
        "            sample_from_model(model, vectorizer, nationalities=nationalities), \r\n",
        "            vectorizer)\r\n",
        "        \r\n",
        "        sample1 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[0]), \r\n",
        "                                  sampled_surnames[0])\r\n",
        "        sample2 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[1]), \r\n",
        "                                  sampled_surnames[1])\r\n",
        "        epoch_bar.set_postfix(sample1=sample1, \r\n",
        "                              sample2=sample2)\r\n",
        "        # move model back to whichever device it should be on\r\n",
        "        model = model.to(args.device)\r\n",
        "        \r\n",
        "        train_bar.n = 0\r\n",
        "        val_bar.n = 0\r\n",
        "        epoch_bar.update()\r\n",
        "        \r\n",
        "except KeyboardInterrupt:\r\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d39ec6ca8624388854bff979147ccc7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training routine', max=200.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afcdcbee127e4ef593de336da8feeb01",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=train', max=60.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9ad0beb0a4f4663a2a76626ec0f70de",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=val', max=12.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmDRzksNOAnJ"
      },
      "source": [
        "### Test Set Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdWq_ecMODxh"
      },
      "source": [
        "### Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8JnrUSqOF1X",
        "outputId": "008c2d08-0264-4bc7-b777-64db44e60ae9"
      },
      "source": [
        "model = model.cpu()\r\n",
        "for index in range(len(vectorizer.nationality_vocab)):\r\n",
        "    nationality = vectorizer.nationality_vocab.lookup_index(index)\r\n",
        "    print(\"Sampled for {}: \".format(nationality))\r\n",
        "    sampled_indices = sample_from_model(model, vectorizer,  \r\n",
        "                                        nationalities=[index] * 3, \r\n",
        "                                        temperature=0.7)\r\n",
        "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\r\n",
        "        print(\"-  \" + sampled_surname)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampled for Arabic: \n",
            "-  Saibon\n",
            "-  Ganim\n",
            "-  Tahan\n",
            "Sampled for Chinese: \n",
            "-  Hong\n",
            "-  Buen\n",
            "-  Jan\n",
            "Sampled for Czech: \n",
            "-  Tallo\n",
            "-  Grovar\n",
            "-  Hallicha\n",
            "Sampled for Dutch: \n",
            "-  Marsen\n",
            "-  Voner\n",
            "-  Schein\n",
            "Sampled for English: \n",
            "-  Lisham\n",
            "-  Harny\n",
            "-  Tadbern\n",
            "Sampled for French: \n",
            "-  Doneart\n",
            "-  Tagin\n",
            "-  Balmat\n",
            "Sampled for German: \n",
            "-  Tchidger\n",
            "-  Fertel\n",
            "-  Schlerlen\n",
            "Sampled for Greek: \n",
            "-  Alsaris\n",
            "-  Slapis\n",
            "-  Atnopos\n",
            "Sampled for Irish: \n",
            "-  Carlige\n",
            "-  Dein\n",
            "-  Coelon\n",
            "Sampled for Italian: \n",
            "-  Mascli\n",
            "-  Abbartan\n",
            "-  Asteri\n",
            "Sampled for Japanese: \n",
            "-  Isura\n",
            "-  Asami\n",
            "-  Iakuta\n",
            "Sampled for Korean: \n",
            "-  Yi\n",
            "-  Ot\n",
            "-  Mon\n",
            "Sampled for Polish: \n",
            "-  Zininov\n",
            "-  Kilokoko\n",
            "-  Mojzan\n",
            "Sampled for Portuguese: \n",
            "-  Ariso\n",
            "-  Mannocha\n",
            "-  Denora\n",
            "Sampled for Russian: \n",
            "-  Dozinev\n",
            "-  Battosman\n",
            "-  Pirkov\n",
            "Sampled for Scottish: \n",
            "-  Winton\n",
            "-  Horren\n",
            "-  Hodron\n",
            "Sampled for Spanish: \n",
            "-  Merni\n",
            "-  Srabo\n",
            "-  Vire\n",
            "Sampled for Vietnamese: \n",
            "-  Ba\n",
            "-  Agr\n",
            "-  Huon\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}