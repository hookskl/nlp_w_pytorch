{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQO/JelB/O3zVIxwoib79s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdRDG3KL5S3a"
      },
      "source": [
        "# Embedding Words and Types\r\n",
        "\r\n",
        "Part of implementing any NLP task involves dealing with different kinds of discrete types. Examples of discrete types are:\r\n",
        "\r\n",
        "* words\r\n",
        "* characters\r\n",
        "* parts-of-speech tags (POS)\r\n",
        "* named entities\r\n",
        "* named entity types\r\n",
        "* parse features \r\n",
        "* items in a product catalog\r\n",
        "\r\n",
        "Any input feature that comes from a finite (or countably finite) set (aka a vocabulary), it is a *discrete type*.\r\n",
        "\r\n",
        "One of the core successes to deep learning in NLP is the method of representing discrete types as dense vectors. \"Representation learning\" or \"embedding\" refer to learning a mapping from one discrete type to a point in a vector space. In the context of words, this mapping is referred to as a *word embedding*. Other embedding methods exist, such as count-based embeddings (TF-IDF). The focus here will be *learning-based* or *prediction-based* embedding methods, where the representations are learned by maximizing an objective for a specific learning task. One such example is predicting a word based on context. These learned embeddings are so quintessential to modern NLP that it can be expected the performance on any NLP task will improve by adding one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu00Qwau5_f3"
      },
      "source": [
        "## Why Learn Embeddings?\r\n",
        "\r\n",
        "Learned embeddings have several advantages over more classical representations, such as count-based methods that are heuristically constructed.\r\n",
        "\r\n",
        "First, they are more computationally efficient since their size does not scale with the size of the vocabularly. Second, count-based methods result in high-dimensional vectors that encode redundant information along many dimensions. Third, very high dimensions lead to problems in fitting machine learning models (*the curse of dimensionality*). Finally, learned representations are more suited to the task at hand, whereas count-based or low dimensional approaches (SVD and PCA) are not necessarily optimized for the relevant task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlELToI6Ciy"
      },
      "source": [
        "### Efficiency of Embeddings\r\n",
        "\r\n",
        "One of the major efficiencies of word embeddings is their size is typically much smaller than those of one-hot or count-based representations. Typical sizes range between 25 and 500 dimensions, usually dicatated by hardware limitations.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHfkXR16Hk7"
      },
      "source": [
        "### Approaches to Learning Word Embeddings\r\n",
        "\r\n",
        "All word embedding methods train with just words in a supervised fashion. This is accomplished by constructing *auxiliary* tasks in which the data is implicitly labeled. Some examples:\r\n",
        "\r\n",
        "* given a sequence of words, predict the new word (also called the *langauge modeling* task)\r\n",
        "* given a sequence of words before and after, predict the missing word\r\n",
        "* give a word, predict words that occur within a window, indepdent of the position\r\n",
        "\r\n",
        "Generally, it's more worthwhile to use a pretrained word embedding and fine-tune than to create one from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G93wPQaf6LjI"
      },
      "source": [
        "### The Practical Use of Pretrained Word Embeddings\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRo30C146Q85"
      },
      "source": [
        "#### Loading Embeddings\r\n",
        "\r\n",
        "Some popular pretrained word embeddings are:\r\n",
        "\r\n",
        "* Word2Vec \r\n",
        "* GLoVe\r\n",
        "* FastText\r\n",
        "\r\n",
        "The typical file format for these embeddings is as follows: each line starts with the word/type that is being embedded and is followed by a sequence of numbers (the vector representation). The length of this sequence is the dimension of the representation (embedding dimension). \r\n",
        "\r\n",
        "A utility class called `PreTrainedWordEmbeddings` is used to load and process embeddings. This class builds an in-memory index of all the word vectors for quick lookups and nearest-neighbor queries using an approximate nearest-neighbor package called `annoy`.\r\n",
        "\r\n",
        "*Example 5-1. Using pretrained word embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m11boiTara_F",
        "outputId": "666a794e-edb5-4d74-a742-8341e6f24645"
      },
      "source": [
        "%%shell\r\n",
        "# download glove model \r\n",
        "wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "unzip glove*.zip\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 20:12:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-01-31 20:12:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-01-31 20:12:31--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.00MB/s    in 6m 34s  \n",
            "\n",
            "2021-01-31 20:19:05 (2.09 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQVBs-g45PPX"
      },
      "source": [
        "# !pip install annoy\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from annoy import AnnoyIndex\r\n",
        "\r\n",
        "class PretrainedEmbeddings(object):\r\n",
        "    def __init__(self, word_to_index, word_vectors):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word_to_index (dict): mapping from word to integers\r\n",
        "            word_vectors (list of numpy arrays)\r\n",
        "        \"\"\"\r\n",
        "        self.word_to_index = word_to_index\r\n",
        "        self.word_vectors = word_vectors\r\n",
        "        self.index_to_word = \\\r\n",
        "            {v: k for k, v in self.word_to_index.items()}\r\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]),\r\n",
        "                                metric='euclidean')\r\n",
        "        for _, i in self.word_to_index.items():\r\n",
        "            self.index.add_item(i, self.word_vectors[i])\r\n",
        "        self.index.build(50)\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_embeddings_file(cls, embedding_file):\r\n",
        "        \"\"\"Instantiate from pretrained vector file.\r\n",
        "\r\n",
        "        Vector file should be the format:\r\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\r\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\r\n",
        "\r\n",
        "        Args:\r\n",
        "            embedding_file (str): location of the file\r\n",
        "        Returns:\r\n",
        "            instance of PreTrainedEmbeddings\r\n",
        "        \"\"\"\r\n",
        "        word_to_index = {}\r\n",
        "        word_vectors = []\r\n",
        "        with open(embedding_file) as fp:\r\n",
        "            for line in fp.readlines():\r\n",
        "                line = line.split(\" \")\r\n",
        "                word = line[0]\r\n",
        "                vec = np.array([float(x) for x in line[1:]])\r\n",
        "\r\n",
        "                word_to_index[word] = len(word_to_index)\r\n",
        "                word_vectors.append(vec)\r\n",
        "        return cls(word_to_index, word_vectors)\r\n",
        "\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60WgtNvHFAS"
      },
      "source": [
        "# load glove embeddings\r\n",
        "embeddings = \\\r\n",
        "    PretrainedEmbeddings.from_embeddings_file('glove.6B.100d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LK-w2CHh4d",
        "outputId": "d04e4d9c-61a1-45af-f896-d0d079a7403a"
      },
      "source": [
        "# print the index of the word 'working'\r\n",
        "print(embeddings.word_to_index['working'])\r\n",
        "# print the word with index 500\r\n",
        "print(embeddings.index_to_word[500])\r\n",
        "# print the word embedding of the word 'working'\r\n",
        "embeddings.word_vectors[500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "working\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.076552 ,  0.17843  , -0.44464  ,  0.085718 ,  0.28268  ,\n",
              "       -0.30546  , -0.30637  ,  0.36632  , -0.19919  ,  0.35636  ,\n",
              "        0.088981 , -0.7717   ,  0.68709  , -0.055057 , -0.47002  ,\n",
              "       -0.52158  ,  0.58331  , -0.32255  , -0.28368  , -0.020115 ,\n",
              "        0.12133  ,  0.63264  ,  0.2717   , -0.61169  , -0.015634 ,\n",
              "       -0.54613  , -0.19113  , -0.77745  , -0.048714 ,  0.38825  ,\n",
              "       -0.68519  ,  0.71731  , -0.075302 , -0.26239  , -0.013498 ,\n",
              "        0.19442  , -0.19793  ,  0.040908 ,  0.78602  , -0.049446 ,\n",
              "       -0.83782  ,  0.10923  , -0.15471  ,  0.12925  , -0.26784  ,\n",
              "        0.045059 , -0.60726  , -0.41779  , -0.063718 , -0.58079  ,\n",
              "       -0.40284  , -0.37669  , -0.18443  ,  1.4475   , -0.15176  ,\n",
              "       -2.215    , -0.22298  , -0.28886  ,  1.3392   ,  0.55239  ,\n",
              "        0.022604 ,  0.70506  , -0.34004  , -0.26593  ,  0.80853  ,\n",
              "        0.26161  ,  0.38258  ,  0.44347  ,  0.37905  ,  0.26225  ,\n",
              "        0.082587 , -0.049931 , -0.19572  , -0.48894  ,  0.20751  ,\n",
              "        0.1884   ,  0.061963 , -0.23657  , -0.48063  , -0.16153  ,\n",
              "        0.49952  ,  0.4713   , -0.20645  ,  0.30818  , -1.7238   ,\n",
              "        0.46214  ,  0.27348  ,  0.15508  , -1.1395   , -0.68702  ,\n",
              "        0.24433  , -0.37973  ,  0.20316  , -0.25226  , -0.30063  ,\n",
              "        0.090957 , -0.21252  ,  0.0059495,  0.6699   , -0.070276 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzl7SGGv7oiv"
      },
      "source": [
        "#### Relationships between word embeddings\r\n",
        "\r\n",
        "One of the features of word embeddings is the encoded syntatic and semantic relationships between various words. For words such as cats and dogs, their respective word vectors are close together relative to other words, such as ducks or elephants.\r\n",
        "\r\n",
        "One way to explore these similarities is the analogy task: $$\\text{Word1 : Word2 :: Word3 : ____}$$\r\n",
        "\r\n",
        "In this task, three words are provided with a missing fourth word. The fourth word is chosen such that it's relationship with the third word is congruent to the relationship between words one and two. With word embeddings, these relationships are encoded spatially. Subtracting the word embedding of `Word2` from `Word1` yields a difference vector that represents this relationship. By adding `Word3` to this difference vector, a new vector is produced that is close to the fourth word. The other word embeddings can be queried using nearest-neighors to find the word embedding most similar to this output vector, solving the analogy task.\r\n",
        "\r\n",
        "*Example 5-2. The analogy task using word embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLPW7un_771z"
      },
      "source": [
        "*Example 5-3. Word embeddings encode many linguistics relationships, as illustrated using the SAT analogy task*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y9WJurB8DdX",
        "outputId": "ac56e483-c677-4c48-d774-d60347bd2866"
      },
      "source": [
        "# Relationship 1: the relationship between gendered nouns and pronouns\r\n",
        "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : he :: woman : she\n",
            "man : he :: woman : her\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur274pQwTp_S",
        "outputId": "33503de6-c4c5-4d6a-a240-78060d9443a9"
      },
      "source": [
        "# Relationship 2: Verb-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly : plane :: sail : ship\n",
            "fly : plane :: sail : vessel\n",
            "fly : plane :: sail : boat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9USYUFWcT6dv",
        "outputId": "39b939f2-e9e2-45d7-9f8c-dc35e01a18e7"
      },
      "source": [
        "# Relationship 3: Noun-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('cat', 'kitten', 'dog')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat : kitten :: dog : puppy\n",
            "cat : kitten :: dog : puppies\n",
            "cat : kitten :: dog : junkyard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owFAP1GceCvz",
        "outputId": "eb438760-85f7-46f5-b287-9b041bad9de6"
      },
      "source": [
        "# Relationship 4: Hypernymy (broader category)\r\n",
        "embeddings.compute_and_print_analogy('blue', 'color', 'dog')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blue : color :: dog : animal\n",
            "blue : color :: dog : pet\n",
            "blue : color :: dog : taste\n",
            "blue : color :: dog : touch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUzTrUyXeChh",
        "outputId": "4db64ada-8a24-4eb9-a3b0-310f02e3a541"
      },
      "source": [
        "# Relationship 5: Meronymy (part-to-whole)\r\n",
        "embeddings.compute_and_print_analogy('toe', 'foot', 'finger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toe : foot :: finger : hand\n",
            "toe : foot :: finger : kept\n",
            "toe : foot :: finger : ground\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwDC9ljveCZy",
        "outputId": "59e74a67-3d52-4072-88fe-1ed977d60781"
      },
      "source": [
        "# Relationship 6: Troponymy (difference in manner)\r\n",
        "embeddings.compute_and_print_analogy('talk', 'communicate', 'read')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "talk : communicate :: read : interpret\n",
            "talk : communicate :: read : communicated\n",
            "talk : communicate :: read : transmit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI1HgdmeeCQi",
        "outputId": "3abeff68-2fc4-4814-9e22-dbe2b1cb98b6"
      },
      "source": [
        "# Relationship 7: Metonymy (convention / figures of speech)\r\n",
        "embeddings.compute_and_print_analogy('blue', 'democrat', 'red')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blue : democrat :: red : republican\n",
            "blue : democrat :: red : congressman\n",
            "blue : democrat :: red : senator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKSPk4oPeCEk",
        "outputId": "2b7ed89f-8a34-4b3c-84af-8a2b141ab0c4"
      },
      "source": [
        "# Relationship 8: Adjectival scales\r\n",
        "embeddings.compute_and_print_analogy('fast', 'fastest', 'young')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fast : fastest :: young : younger\n",
            "fast : fastest :: young : sixth\n",
            "fast : fastest :: young : fifth\n",
            "fast : fastest :: young : seventh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRtOACU8HsB"
      },
      "source": [
        "*5-4. An example illustrating the danger of using cooccurrences to encode meaning---sometimes they do not!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Ufl4Ag8Tbs",
        "outputId": "79b944c3-4899-4886-e759-b01a43b7359a"
      },
      "source": [
        "embeddings.compute_and_print_analogy('fast', 'fastest', 'small')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fast : fastest :: small : smallest\n",
            "fast : fastest :: small : largest\n",
            "fast : fastest :: small : among\n",
            "fast : fastest :: small : quarters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-q3MKxT8dsx"
      },
      "source": [
        "*Example 5-5. Watch out for protected attributes such as gender encoded in word embeddings. This can introduce unwanted biases in downstream models.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tynKBZs68mGb",
        "outputId": "cb48fef8-df1f-49b0-e709-b48b081f2245"
      },
      "source": [
        "embeddings.compute_and_print_analogy('man', 'king', 'woman')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : king :: woman : queen\n",
            "man : king :: woman : monarch\n",
            "man : king :: woman : throne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdtzusZI8q4w"
      },
      "source": [
        "*Example 5-6. Cultural gender bias encoded in vector analogy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbf8daZu8v2_",
        "outputId": "52815de0-0361-423e-83e2-36eeb5538859"
      },
      "source": [
        "embeddings.compute_and_print_analogy('man', 'doctor', 'woman')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : doctor :: woman : nurse\n",
            "man : doctor :: woman : physician\n",
            "man : doctor :: woman : doctors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwYpK1efXnr"
      },
      "source": [
        "The above example shows how codified cultural biases can manifest within word embeddings and is something practioners should be aware of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHRwKBAa8zfa"
      },
      "source": [
        "## Exampled: Learning the Continuous Bag of Words Embeddings\r\n",
        "\r\n",
        "This example walks through one of the most famous methods for constructing word embeddings, the Word2Vec Continuous-Bag-of-Words (CBOW) model. The CBOW model is a multi-classification task that scans over texts of words, creating a \"context\" window of words. The center word of this window is removed and then the window is used to classify the missing word, essentially filling in the blank.\r\n",
        "\r\n",
        "A PyTorch module `nn.Embedding` layer will be used to implement the CBOW model. The `Embedding` layer encapsulates an embedding matrix and provides a mapping from a token's integer ID to a vector that is used in the neural network computation. When the model is training and updating weights, these vectors will also be updated. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHNOrJn836G"
      },
      "source": [
        "### The Frankenstein Dataset\r\n",
        "\r\n",
        "The Frankenstein Dataset is a digitized version of Mary Shelley's novel *Frankenstein* and available at [Project Gutenburg](https://bit.ly/2T5iU8J).\r\n",
        "\r\n",
        "Using the raw text file, the test is split into sentences and then each sentence is converted to lowercase and stripped of all punctuation. This is handled using NLTK's `Punkt` tokenizer. With the text preprocessed a list of tokens can be retrieved by splitting on whitespace.\r\n",
        "\r\n",
        "The data is then enumerated as a sequence of windows so that the CBOW model can be optimized. The list of tokens for each sentence is iterated over, grouping them into windows of a specific window size. As a quick example with window size of 2:\r\n",
        "\r\n",
        "* i pitied frankenstein my pity amounted to horror i abhorred myself\r\n",
        "* *i* <ins>pitied frankenstein</ins> my pity amounted to horror i abhorred myself\r\n",
        "* <ins>i</ins> *pitied* <ins>frankenstein my</ins> pity amounted to horror i abhorred myself\r\n",
        "* <ins>i pitied</ins> *frankenstein* <ins>my pity</ins> amounted to horror i abhorred myself\r\n",
        "* i <ins>pitied frankenstein</ins> *my* <ins>pity amounted</ins> to horror i abhorred myself\r\n",
        "\r\n",
        "Above shows the sliding context window over the processed sentence \"i pitied frankenstein my pity amounted to horror i abhorred myself\", with the *target* italicized and the <ins>context window</ins> underlined.\r\n",
        "\r\n",
        "Once the data is processed, the familiar step of splitting the data into train, validaiton, and test is done. \r\n",
        "\r\n",
        "The data with splits is loaded into a Pandas `DataFrame` and indexed in the `CBOWDataset` class. The `__getitem__()` utilizes the `Vectorizer` to convert the context into a vector. The target word is converted to an integer using the `Vocabulary`.\r\n",
        "\r\n",
        "*Example 5-7. Constructing a dataset class for the CBOW task*\r\n",
        "\r\n",
        "```\r\n",
        "class CBOWDataset(Dataset):\r\n",
        "    # ...existing implementation from Example 3-15\r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\r\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            cbow_csv (str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of CBOWDataset\r\n",
        "        \"\"\"\r\n",
        "        cbow_df = pd.read_csv(cbow_csv)\r\n",
        "        train_cbow_df = cbow_df[cbow_df.split=='train']\r\n",
        "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point\r\n",
        "        Returns:\r\n",
        "            a dict with features (x_data) and label (y_target)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "\r\n",
        "        context_vector = \\\r\n",
        "            self._vectorizer.vectorize(row.context, self._max_seq_length)\r\n",
        "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\r\n",
        "\r\n",
        "        return {'x_data': context_vector,\r\n",
        "                'y_target': target_index}\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROVOvUG9FmY"
      },
      "source": [
        "### Vocabularly, Vectorizer, DataLoader\r\n",
        "\r\n",
        "*Exampled 5-8. A Vectorizer for the CBOW data*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWlBD2k9Q0H"
      },
      "source": [
        "### The CBOWClassifier Model\r\n",
        "\r\n",
        "*Example 5-9. The CBOWClassifier model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITb29y39X_N"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-10. Arguments to the CBOW training script*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvFE6dvk-EeD"
      },
      "source": [
        "### Model Evaluation and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk-tU26U-HWe"
      },
      "source": [
        "## Example: Transfer Learning Using Pretrained Embeddings for Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQQgJwJ-pAm"
      },
      "source": [
        "### The AG News Dataset\r\n",
        "\r\n",
        "*Example 5-11. The NewsDataset.__getitem__() method*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAPLV_N9_PyE"
      },
      "source": [
        "### Vocabulary, Vectorizer, and DataLoader\r\n",
        "\r\n",
        "*Exampled 5-12. Implementing a Vectorizer for the AG News dataset*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGGRIPP_Z_3"
      },
      "source": [
        "### The NewsClassifier Model\r\n",
        "\r\n",
        "*Example 5-13. Selecting a subset of the word embeddings based on the vocabulary*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n",
        "\r\n",
        "*Example 5-14. Implementing the NewsClassifier*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxRd0IAALMM"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-15. Arguments to the CNN NewsClassifier using pretrained embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAdlrfokAT8P"
      },
      "source": [
        "### Model Evaluation and Prediction\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2ZVbJSAXP-"
      },
      "source": [
        "#### Evaluating on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT10jwLDAZsh"
      },
      "source": [
        "#### Predicting the category of novel news headlines\r\n",
        "\r\n",
        "*Example 5-16. Predicting with the trained model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    }
  ]
}