{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMONiEA4WFVeahfOB6d77/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdRDG3KL5S3a"
      },
      "source": [
        "# Embedding Words and Types\r\n",
        "\r\n",
        "Part of implementing any NLP task involves dealing with different kinds of discrete types. Examples of discrete types are:\r\n",
        "\r\n",
        "* words\r\n",
        "* characters\r\n",
        "* parts-of-speech tags (POS)\r\n",
        "* named entities\r\n",
        "* named entity types\r\n",
        "* parse features \r\n",
        "* items in a product catalog\r\n",
        "\r\n",
        "Any input feature that comes from a finite (or countably finite) set (aka a vocabulary), it is a *discrete type*.\r\n",
        "\r\n",
        "One of the core successes to deep learning in NLP is the method of representing discrete types as dense vectors. \"Representation learning\" or \"embedding\" refer to learning a mapping from one discrete type to a point in a vector space. In the context of words, this mapping is referred to as a *word embedding*. Other embedding methods exist, such as count-based embeddings (TF-IDF). The focus here will be *learning-based* or *prediction-based* embedding methods, where the representations are learned by maximizing an objective for a specific learning task. One such example is predicting a word based on context. These learned embeddings are so quintessential to modern NLP that it can be expected the performance on any NLP task will improve by adding one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu00Qwau5_f3"
      },
      "source": [
        "## Why Learn Embeddings?\r\n",
        "\r\n",
        "Learned embeddings have several advantages over more classical representations, such as count-based methods that are heuristically constructed.\r\n",
        "\r\n",
        "First, they are more computationally efficient since their size does not scale with the size of the vocabularly. Second, count-based methods result in high-dimensional vectors that encode redundant information along many dimensions. Third, very high dimensions lead to problems in fitting machine learning models (*the curse of dimensionality*). Finally, learned representations are more suited to the task at hand, whereas count-based or low dimensional approaches (SVD and PCA) are not necessarily optimized for the relevant task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlELToI6Ciy"
      },
      "source": [
        "### Efficiency of Embeddings\r\n",
        "\r\n",
        "One of the major efficiencies of word embeddings is their size is typically much smaller than those of one-hot or count-based representations. Typical sizes range between 25 and 500 dimensions, usually dicatated by hardware limitations.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHfkXR16Hk7"
      },
      "source": [
        "### Approaches to Learning Word Embeddings\r\n",
        "\r\n",
        "All word embedding methods train with just words in a supervised fashion. This is accomplished by constructing *auxiliary* tasks in which the data is implicitly labeled. Some examples:\r\n",
        "\r\n",
        "* given a sequence of words, predict the new word (also called the *langauge modeling* task)\r\n",
        "* given a sequence of words before and after, predict the missing word\r\n",
        "* give a word, predict words that occur within a window, indepdent of the position\r\n",
        "\r\n",
        "Generally, it's more worthwhile to use a pretrained word embedding and fine-tune than to create one from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G93wPQaf6LjI"
      },
      "source": [
        "### The Practical Use of Pretrained Word Embeddings\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRo30C146Q85"
      },
      "source": [
        "#### Loading Embeddings\r\n",
        "\r\n",
        "Some popular pretrained word embeddings are:\r\n",
        "\r\n",
        "* Word2Vec \r\n",
        "* GLoVe\r\n",
        "* FastText\r\n",
        "\r\n",
        "The typical file format for these embeddings is as follows: each line starts with the word/type that is being embedded and is followed by a sequence of numbers (the vector representation). The length of this sequence is the dimension of the representation (embedding dimension). \r\n",
        "\r\n",
        "A utility class called `PreTrainedWordEmbeddings` is used to load and process embeddings. This class builds an in-memory index of all the word vectors for quick lookups and nearest-neighbor queries using an approximate nearest-neighbor package called `annoy`.\r\n",
        "\r\n",
        "*Example 5-1. Using pretrained word embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m11boiTara_F",
        "outputId": "0e8b50ed-61ab-4a65-e125-7062e500991d"
      },
      "source": [
        "%%shell\r\n",
        "# download glove model \r\n",
        "wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "unzip glove*.zip\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 17:03:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-02 17:03:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-02 17:03:37--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.20MB/s    in 6m 31s  \n",
            "\n",
            "2021-02-02 17:10:08 (2.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQVBs-g45PPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41416721-dbee-4a19-9f9c-81ba46d9480e"
      },
      "source": [
        "!pip install annoy\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from annoy import AnnoyIndex\r\n",
        "\r\n",
        "class PretrainedEmbeddings(object):\r\n",
        "    def __init__(self, word_to_index, word_vectors):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word_to_index (dict): mapping from word to integers\r\n",
        "            word_vectors (list of numpy arrays)\r\n",
        "        \"\"\"\r\n",
        "        self.word_to_index = word_to_index\r\n",
        "        self.word_vectors = word_vectors\r\n",
        "        self.index_to_word = \\\r\n",
        "            {v: k for k, v in self.word_to_index.items()}\r\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]),\r\n",
        "                                metric='euclidean')\r\n",
        "        for _, i in self.word_to_index.items():\r\n",
        "            self.index.add_item(i, self.word_vectors[i])\r\n",
        "        self.index.build(50)\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_embeddings_file(cls, embedding_file):\r\n",
        "        \"\"\"Instantiate from pretrained vector file.\r\n",
        "\r\n",
        "        Vector file should be the format:\r\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\r\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\r\n",
        "\r\n",
        "        Args:\r\n",
        "            embedding_file (str): location of the file\r\n",
        "        Returns:\r\n",
        "            instance of PreTrainedEmbeddings\r\n",
        "        \"\"\"\r\n",
        "        word_to_index = {}\r\n",
        "        word_vectors = []\r\n",
        "        with open(embedding_file) as fp:\r\n",
        "            for line in fp.readlines():\r\n",
        "                line = line.split(\" \")\r\n",
        "                word = line[0]\r\n",
        "                vec = np.array([float(x) for x in line[1:]])\r\n",
        "\r\n",
        "                word_to_index[word] = len(word_to_index)\r\n",
        "                word_vectors.append(vec)\r\n",
        "        return cls(word_to_index, word_vectors)\r\n",
        "\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\r\u001b[K     |▌                               | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 16.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 13.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 12.3MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 12.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 12.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=385815 sha256=cd24d372b2671813590b1e61b9c96d7f149766db9d7d25d82d2c0b39251468ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60WgtNvHFAS"
      },
      "source": [
        "# load glove embeddings\r\n",
        "embeddings = \\\r\n",
        "    PretrainedEmbeddings.from_embeddings_file('glove.6B.100d.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LK-w2CHh4d",
        "outputId": "1839c7f1-f82e-4faa-9515-01c6284305f1"
      },
      "source": [
        "# print the index of the word 'working'\r\n",
        "print(embeddings.word_to_index['working'])\r\n",
        "# print the word with index 500\r\n",
        "print(embeddings.index_to_word[500])\r\n",
        "# print the word embedding of the word 'working'\r\n",
        "embeddings.word_vectors[500]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "working\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.076552 ,  0.17843  , -0.44464  ,  0.085718 ,  0.28268  ,\n",
              "       -0.30546  , -0.30637  ,  0.36632  , -0.19919  ,  0.35636  ,\n",
              "        0.088981 , -0.7717   ,  0.68709  , -0.055057 , -0.47002  ,\n",
              "       -0.52158  ,  0.58331  , -0.32255  , -0.28368  , -0.020115 ,\n",
              "        0.12133  ,  0.63264  ,  0.2717   , -0.61169  , -0.015634 ,\n",
              "       -0.54613  , -0.19113  , -0.77745  , -0.048714 ,  0.38825  ,\n",
              "       -0.68519  ,  0.71731  , -0.075302 , -0.26239  , -0.013498 ,\n",
              "        0.19442  , -0.19793  ,  0.040908 ,  0.78602  , -0.049446 ,\n",
              "       -0.83782  ,  0.10923  , -0.15471  ,  0.12925  , -0.26784  ,\n",
              "        0.045059 , -0.60726  , -0.41779  , -0.063718 , -0.58079  ,\n",
              "       -0.40284  , -0.37669  , -0.18443  ,  1.4475   , -0.15176  ,\n",
              "       -2.215    , -0.22298  , -0.28886  ,  1.3392   ,  0.55239  ,\n",
              "        0.022604 ,  0.70506  , -0.34004  , -0.26593  ,  0.80853  ,\n",
              "        0.26161  ,  0.38258  ,  0.44347  ,  0.37905  ,  0.26225  ,\n",
              "        0.082587 , -0.049931 , -0.19572  , -0.48894  ,  0.20751  ,\n",
              "        0.1884   ,  0.061963 , -0.23657  , -0.48063  , -0.16153  ,\n",
              "        0.49952  ,  0.4713   , -0.20645  ,  0.30818  , -1.7238   ,\n",
              "        0.46214  ,  0.27348  ,  0.15508  , -1.1395   , -0.68702  ,\n",
              "        0.24433  , -0.37973  ,  0.20316  , -0.25226  , -0.30063  ,\n",
              "        0.090957 , -0.21252  ,  0.0059495,  0.6699   , -0.070276 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzl7SGGv7oiv"
      },
      "source": [
        "#### Relationships between word embeddings\r\n",
        "\r\n",
        "One of the features of word embeddings is the encoded syntatic and semantic relationships between various words. For words such as cats and dogs, their respective word vectors are close together relative to other words, such as ducks or elephants.\r\n",
        "\r\n",
        "One way to explore these similarities is the analogy task: $$\\text{Word1 : Word2 :: Word3 : ____}$$\r\n",
        "\r\n",
        "In this task, three words are provided with a missing fourth word. The fourth word is chosen such that it's relationship with the third word is congruent to the relationship between words one and two. With word embeddings, these relationships are encoded spatially. Subtracting the word embedding of `Word2` from `Word1` yields a difference vector that represents this relationship. By adding `Word3` to this difference vector, a new vector is produced that is close to the fourth word. The other word embeddings can be queried using nearest-neighors to find the word embedding most similar to this output vector, solving the analogy task.\r\n",
        "\r\n",
        "*Example 5-2. The analogy task using word embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLPW7un_771z"
      },
      "source": [
        "*Example 5-3. Word embeddings encode many linguistics relationships, as illustrated using the SAT analogy task*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y9WJurB8DdX",
        "outputId": "914f429c-2e44-4172-d91e-c8d55f19b942"
      },
      "source": [
        "# Relationship 1: the relationship between gendered nouns and pronouns\r\n",
        "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : he :: woman : she\n",
            "man : he :: woman : her\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur274pQwTp_S",
        "outputId": "36917946-cdae-4c6e-bacc-3cc02ff0801c"
      },
      "source": [
        "# Relationship 2: Verb-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly : plane :: sail : ship\n",
            "fly : plane :: sail : vessel\n",
            "fly : plane :: sail : boat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9USYUFWcT6dv",
        "outputId": "e5e9687c-558a-4b82-f499-1b185c22f242"
      },
      "source": [
        "# Relationship 3: Noun-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('cat', 'kitten', 'dog')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat : kitten :: dog : puppy\n",
            "cat : kitten :: dog : puppies\n",
            "cat : kitten :: dog : junkyard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owFAP1GceCvz",
        "outputId": "dd089465-149d-47d4-88ea-d0230a8cca44"
      },
      "source": [
        "# Relationship 4: Hypernymy (broader category)\r\n",
        "embeddings.compute_and_print_analogy('blue', 'color', 'dog')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blue : color :: dog : animal\n",
            "blue : color :: dog : pet\n",
            "blue : color :: dog : taste\n",
            "blue : color :: dog : touch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUzTrUyXeChh",
        "outputId": "e15cba19-8f15-45c4-9c70-bd4fc82febe4"
      },
      "source": [
        "# Relationship 5: Meronymy (part-to-whole)\r\n",
        "embeddings.compute_and_print_analogy('toe', 'foot', 'finger')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toe : foot :: finger : hand\n",
            "toe : foot :: finger : kept\n",
            "toe : foot :: finger : ground\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwDC9ljveCZy",
        "outputId": "ff00d5d8-016c-4ded-ae19-847b7316993c"
      },
      "source": [
        "# Relationship 6: Troponymy (difference in manner)\r\n",
        "embeddings.compute_and_print_analogy('talk', 'communicate', 'read')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "talk : communicate :: read : interpret\n",
            "talk : communicate :: read : communicated\n",
            "talk : communicate :: read : transmit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI1HgdmeeCQi",
        "outputId": "b26fd93c-6f5b-495a-8214-98e2254d2825"
      },
      "source": [
        "# Relationship 7: Metonymy (convention / figures of speech)\r\n",
        "embeddings.compute_and_print_analogy('blue', 'democrat', 'red')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blue : democrat :: red : republican\n",
            "blue : democrat :: red : congressman\n",
            "blue : democrat :: red : senator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKSPk4oPeCEk",
        "outputId": "e6614414-821c-4971-818f-985f92ec785b"
      },
      "source": [
        "# Relationship 8: Adjectival scales\r\n",
        "embeddings.compute_and_print_analogy('fast', 'fastest', 'young')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fast : fastest :: young : younger\n",
            "fast : fastest :: young : sixth\n",
            "fast : fastest :: young : fifth\n",
            "fast : fastest :: young : seventh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRtOACU8HsB"
      },
      "source": [
        "*5-4. An example illustrating the danger of using cooccurrences to encode meaning---sometimes they do not!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Ufl4Ag8Tbs",
        "outputId": "59d92d69-4bb9-404d-f3c9-3c1c6dc4d2b3"
      },
      "source": [
        "embeddings.compute_and_print_analogy('fast', 'fastest', 'small')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fast : fastest :: small : smallest\n",
            "fast : fastest :: small : largest\n",
            "fast : fastest :: small : among\n",
            "fast : fastest :: small : quarters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-q3MKxT8dsx"
      },
      "source": [
        "*Example 5-5. Watch out for protected attributes such as gender encoded in word embeddings. This can introduce unwanted biases in downstream models.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tynKBZs68mGb",
        "outputId": "925da470-2326-419e-bcf9-61f3ace17320"
      },
      "source": [
        "embeddings.compute_and_print_analogy('man', 'king', 'woman')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : king :: woman : queen\n",
            "man : king :: woman : monarch\n",
            "man : king :: woman : throne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdtzusZI8q4w"
      },
      "source": [
        "*Example 5-6. Cultural gender bias encoded in vector analogy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbf8daZu8v2_",
        "outputId": "f95ee668-d923-4c7c-cb23-c1b4aaf05f1b"
      },
      "source": [
        "embeddings.compute_and_print_analogy('man', 'doctor', 'woman')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : doctor :: woman : nurse\n",
            "man : doctor :: woman : physician\n",
            "man : doctor :: woman : doctors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwYpK1efXnr"
      },
      "source": [
        "The above example shows how codified cultural biases can manifest within word embeddings and is something practioners should be aware of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHRwKBAa8zfa"
      },
      "source": [
        "## Exampled: Learning the Continuous Bag of Words Embeddings\r\n",
        "\r\n",
        "This example walks through one of the most famous methods for constructing word embeddings, the Word2Vec Continuous-Bag-of-Words (CBOW) model. The CBOW model is a multi-classification task that scans over texts of words, creating a \"context\" window of words. The center word of this window is removed and then the window is used to classify the missing word, essentially filling in the blank.\r\n",
        "\r\n",
        "A PyTorch module `nn.Embedding` layer will be used to implement the CBOW model. The `Embedding` layer encapsulates an embedding matrix and provides a mapping from a token's integer ID to a vector that is used in the neural network computation. When the model is training and updating weights, these vectors will also be updated. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHNOrJn836G"
      },
      "source": [
        "### The Frankenstein Dataset\r\n",
        "\r\n",
        "The Frankenstein Dataset is a digitized version of Mary Shelley's novel *Frankenstein* and available at [Project Gutenburg](https://bit.ly/2T5iU8J).\r\n",
        "\r\n",
        "Using the raw text file, the test is split into sentences and then each sentence is converted to lowercase and stripped of all punctuation. This is handled using NLTK's `Punkt` tokenizer. With the text preprocessed a list of tokens can be retrieved by splitting on whitespace.\r\n",
        "\r\n",
        "The data is then enumerated as a sequence of windows so that the CBOW model can be optimized. The list of tokens for each sentence is iterated over, grouping them into windows of a specific window size. As a quick example with window size of 2:\r\n",
        "\r\n",
        "* i pitied frankenstein my pity amounted to horror i abhorred myself\r\n",
        "* *i* <ins>pitied frankenstein</ins> my pity amounted to horror i abhorred myself\r\n",
        "* <ins>i</ins> *pitied* <ins>frankenstein my</ins> pity amounted to horror i abhorred myself\r\n",
        "* <ins>i pitied</ins> *frankenstein* <ins>my pity</ins> amounted to horror i abhorred myself\r\n",
        "* i <ins>pitied frankenstein</ins> *my* <ins>pity amounted</ins> to horror i abhorred myself\r\n",
        "\r\n",
        "Above shows the sliding context window over the processed sentence \"i pitied frankenstein my pity amounted to horror i abhorred myself\", with the *target* italicized and the <ins>context window</ins> underlined.\r\n",
        "\r\n",
        "Once the data is processed, the familiar step of splitting the data into train, validaiton, and test is done. \r\n",
        "\r\n",
        "The data with splits is loaded into a Pandas `DataFrame` and indexed in the `CBOWDataset` class. The `__getitem__()` utilizes the `Vectorizer` to convert the context into a vector. The target word is converted to an integer using the `Vocabulary`.\r\n",
        "\r\n",
        "*Example 5-7. Constructing a dataset class for the CBOW task*\r\n",
        "\r\n",
        "```\r\n",
        "class CBOWDataset(Dataset):\r\n",
        "    # ...existing implementation from Example 3-15\r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\r\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            cbow_csv (str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of CBOWDataset\r\n",
        "        \"\"\"\r\n",
        "        cbow_df = pd.read_csv(cbow_csv)\r\n",
        "        train_cbow_df = cbow_df[cbow_df.split=='train']\r\n",
        "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point\r\n",
        "        Returns:\r\n",
        "            a dict with features (x_data) and label (y_target)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "\r\n",
        "        context_vector = \\\r\n",
        "            self._vectorizer.vectorize(row.context, self._max_seq_length)\r\n",
        "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\r\n",
        "\r\n",
        "        return {'x_data': context_vector,\r\n",
        "                'y_target': target_index}\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROVOvUG9FmY"
      },
      "source": [
        "### Vocabularly, Vectorizer, DataLoader\r\n",
        "\r\n",
        "The pipeline from text to vectorized minibatch is largely unchanged for the CBOW classification task. However, the `Vectorizer` does not construct one-hot vectors. Instead, a vector of integers representing the indices of the context is constructed and returned.\r\n",
        "\r\n",
        "*Exampled 5-8. A Vectorizer for the CBOW data*\r\n",
        "\r\n",
        "```\r\n",
        "class CBOWVectorizer(object):\r\n",
        "    \"\"\"The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\r\n",
        "\r\n",
        "    def vectorize(self, context, vector_length=-1):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            context (str): the string of words separated by a space\r\n",
        "            vector_length (int): an argument for forcing the length of index vector\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        indices = \\\r\n",
        "            [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices)\r\n",
        "        \r\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        out_vector[:len(indices)] = indices\r\n",
        "        out_vector[len(indicies):] = self.cbow_vocab.mask_index\r\n",
        "\r\n",
        "        return out_vector\r\n",
        "```\r\n",
        "\r\n",
        "An important note about the implementation is if the number of tokens in the context is less than the max length, the remaining entries are filled with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWlBD2k9Q0H"
      },
      "source": [
        "### The CBOWClassifier Model\r\n",
        "\r\n",
        "This model has three essential steps:\r\n",
        "\r\n",
        "1. indices representing the words of the context are used with an `Embedding` layer to create vectors for each word in the context\r\n",
        "2. the goal is to combine the vectors in some way such that it captures the overall context. One option is to sum over the vectors (done here) but other arithmetic options are valid.\r\n",
        "3. the context vector is used with a `Linear` layer to compute a prediction vector. This represents a probability distribution over the entire vocab. The missing word (target) is predicted using the largest value from this vector.\r\n",
        "\r\n",
        "The `Embedding` layer is parameterized by two numbers: the number of embeddings (size of the vocab) and the size of the embeddings (embedding dimension). A third argument is used, `padding_idx`, which is used as a sentinel value to the embedding layer for situations where the data points may not all be the same length. \r\n",
        "\r\n",
        "*Example 5-9. The CBOWClassifier model*\r\n",
        "\r\n",
        "```\r\n",
        "class CBOWClassifier(nn.Module):\r\n",
        "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            vocabulary_size (int): number of vocabulary items, controls the \r\n",
        "                number of embeddings and prediction vector size\r\n",
        "            embedding_size (int): size of the embeddings\r\n",
        "            padding_idx (int): default 0; Embedding will not use this index\r\n",
        "        \"\"\"\r\n",
        "        super(CBOWClassifier, self).__init__()\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabularly_size,\r\n",
        "                                      embedding_dim=embedding_size,\r\n",
        "                                      padding_idx=padding_idx)\r\n",
        "        self.fc1 = nn.Linear(in_features=embedding_size,\r\n",
        "                             out_features=vocabulary_size)\r\n",
        "    \r\n",
        "    def forward(self, x_in, apply_softmax=False):\r\n",
        "        \"\"\"The forward pass of the classifier\r\n",
        "\r\n",
        "        Args:\r\n",
        "            x_in (torch.Tensor): an input data tensor\r\n",
        "                x_in.shape should be (batch, input_dim)\r\n",
        "            apply_softmax (bool): a flag for the softmax activation\r\n",
        "                should be False if used with the cross-entropy losses\r\n",
        "        Returns:\r\n",
        "            the resulting tensor\r\n",
        "                tensor.shape should be (batch, output_dim)\r\n",
        "        \"\"\"\r\n",
        "        x_embedded_sum = self.embedding(x_in).sum(dim=1)\r\n",
        "        y_out = self.fc1(x_embedded_sum)\r\n",
        "\r\n",
        "        if apply_softmax:\r\n",
        "            y_out = F.softmax(y_out, dim=1)\r\n",
        "\r\n",
        "        return y_out \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITb29y39X_N"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "The training routine remains unchanged, from previous examples.\r\n",
        "\r\n",
        "*Example 5-10. Arguments to the CBOW training script*\r\n",
        "\r\n",
        "```\r\n",
        "args = Namespace(\r\n",
        "    # Data and path information\r\n",
        "    cbow_csv=\"data/books/frankenstein_with_splits.csv\",\r\n",
        "    vectorizer_file=\"vectorizer.json\",\r\n",
        "    model_state_file=\"model.pth\",\r\n",
        "    save_dir=\"model_storage/ch5/cbow\",\r\n",
        "    # Model hyperparameters\r\n",
        "    embedding_size=300,\r\n",
        "    # Training hyperparameters\r\n",
        "    seed=1337,\r\n",
        "    num_epochs=100,\r\n",
        "    learning_rate=0.001,\r\n",
        "    batch_size=128,\r\n",
        "    early_stopping_criteria=5,\r\n",
        "    # Runtime options omitted\r\n",
        ")\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvFE6dvk-EeD"
      },
      "source": [
        "### Model Evaluation and Prediction\r\n",
        "\r\n",
        "The CBOW model is evaluated on how well it can predict a missing word given a context window. This implementation has only around a 15% accuracy, which is far under the original. Part of this is due to some missing components that add significant complexity, but boost performance. Additionally, the dataset is only around 70,000 words, far below the amount to properly learn word contexts and relationships. A state-of-the-art model requires terabytes of data. \r\n",
        "\r\n",
        "A more practical approach is to use an existing model that has been trained on one task and use as an initializer for a new model. This approach is known as *transfer learning*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk-tU26U-HWe"
      },
      "source": [
        "## Example: Transfer Learning Using Pretrained Embeddings for Document Classification\r\n",
        "\r\n",
        "This example will walk through:\r\n",
        "\r\n",
        "* loading pretrained word embeddings\r\n",
        "* fine-tuning these embeddings on the AG News Dataset\r\n",
        "* using these embeddings in a CNN to classify categories based on headlines\r\n",
        "\r\n",
        "To model the sequences of words the news dataset, the `SequenceVocabulary` class will be utilized. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQQgJwJ-pAm"
      },
      "source": [
        "### The AG News Dataset\r\n",
        "\r\n",
        "The AG news dataset is a collection of more than one million news articles. A trimmed down version will be used here, 120,000 articles split evenly between four categories: Sports, Science/Technology, Word, and Business. Additionally, only the article headline will be used to predict which category the article belongs to. \r\n",
        "\r\n",
        "The text is preprocessed by removing punctuation, adding spaces around punctuation, and converting the text to lowercase. The processsed data is split into the familiar train, validation, and test sets such that the class labels are evenly distributed across the three splits.\r\n",
        "\r\n",
        "The `__getitem__()` method retrieves the string representing the input to the model from a specific row in the dataset, vectorized by the `Vectorizer`, and paired with the integer representing the news category.\r\n",
        "\r\n",
        "*Example 5-11. The* `NewsDataset.__getitem__()` *method*\r\n",
        "\r\n",
        "```\r\n",
        "class NewsDataset(Dataset):\r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_make_vectorizer(cls, news_csv):\r\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            news_csv (str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of NewsDataset\r\n",
        "        \"\"\"\r\n",
        "        news_df = pd.read_csv(news_csv)\r\n",
        "        train_news_df = news_df[news_df.split=='train']\r\n",
        "        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point\r\n",
        "        Returns:\r\n",
        "            a dict holding the data point's features (x_data) and label (y_target)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "\r\n",
        "        title_vector = \\\r\n",
        "            self._vectorizer.vectorize(row.title, self._max_seq_length)\r\n",
        "        \r\n",
        "        category_index = \\\r\n",
        "            self._vectorizer.category_vocab.lookup_token(row.category)\r\n",
        "\r\n",
        "        return {'x_data': title_vector,\r\n",
        "                'y_target': category_index}\r\n",
        "                \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAPLV_N9_PyE"
      },
      "source": [
        "### Vocabulary, Vectorizer, and DataLoader\r\n",
        "\r\n",
        "The `SequenceVocabulary` (a subclass of the `Vocabularly` class) bundles four special tokens used for sequence data:\r\n",
        "\r\n",
        "* `UNK` (unknown) - allows the model to learn a representation for rare words so it can handle words not seen at test time\r\n",
        "* `MASK` - sentinel token for the `Embedding` layer and loss calculations when the sequences have variable length\r\n",
        "* `BEGIN-OF-SEQUENCE` and `END-OF-SEQUENCE` - marks the boundaries for a sequence \r\n",
        "\r\n",
        "To illustrate, take the toy example \"Jerry is happy\". Letting this be the entire corpus, the `SequenceVocbulary` would then be:\r\n",
        "\r\n",
        "```\r\n",
        "SequenceVocabulary\r\n",
        "{\r\n",
        "'<MASK>': 0,\r\n",
        "'<UNK>': 1,\r\n",
        "'<BEGIN-OF-SEQUENCE>': 2,\r\n",
        "'<END-OF-SEQUENCE>': 3,\r\n",
        "'is': 4,\r\n",
        "'happy': 5\r\n",
        "}\r\n",
        "```\r\n",
        "\r\n",
        "Here, \"Jerry\" is treated as a \"rare\" word for illustrative purposes. The sequence is then encoded as follows:\r\n",
        "\r\n",
        "* Step 0: \"Jerry is happy\"\r\n",
        "* Step 1: map words to integers -> [1, 4, 5]\r\n",
        "* Step 2: add boundary tokens -> [2, 1, 4, 5, 3]\r\n",
        "* Step 3: pad sequence for consistent length -> [2, 1, 4, 5, 3, 0, 0]\r\n",
        "\r\n",
        "The second part of turning text to vectorized minibatch is the `Vectorizer`, which instantiates and encapsulates the use of the `SequenceVocabulary`. This implementation follows a previous example that restricts the total set of words by counting and retaining only those above a certain frequency. Limiting low frequency words assists the model by removing noise and also lowers the model's memory usage.\r\n",
        "\r\n",
        "Once instantiated, the `Vectorizer`'s `vectorize()` method takes a news title and returns a vector that is as long as the longest title in the dataset. This method stores the maximum sequence length locally. Normally, the dataset tracks the max sequence length, and at inference time, the length of the test sequence is taken as the length of the vector. However, because the model used is a CNN model, a static length size is maintained at inference time. The outputs are zero-padded vectors of integers, representing the words of the input sequence. These sequences are also supplemented with the being and end tokens described above. \r\n",
        "\r\n",
        "*Example 5-12. Implementing a Vectorizer for the AG News dataset*\r\n",
        "\r\n",
        "```\r\n",
        "class NewsVectorizer(object):\r\n",
        "    def vectorize(self, title, vector_length=-1):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            title (str): the string of the words separated by spaces\r\n",
        "            vector_length (int): forces the length of the index vector\r\n",
        "        Returns:\r\n",
        "            the vectorized title (numpy.array)\r\n",
        "        \"\"\"\r\n",
        "        indices = [self.title_vocab.begin_seq_index]\r\n",
        "        indices.extend(self.title_vocab.lookup_token(token)\r\n",
        "                       for token in title.split(\" \"))\r\n",
        "        indices.append(self.title_vocab.end_seq_index)\r\n",
        "\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices)\r\n",
        "\r\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        out_vector[:len(indices)] = indices\r\n",
        "        out_vector[len(indices):] = self.title_vocab.mask_index\r\n",
        "\r\n",
        "        return out_vector\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_dataframe(cls, news_df, cutoff=25):\r\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\r\n",
        "\r\n",
        "        Args:\r\n",
        "            news_df (pandas.DataFrame): the target dataset\r\n",
        "            cutoff (int): frequency threshold for including in Vocabulary\r\n",
        "        Returns:\r\n",
        "            an instance of the NewsVectorizer\r\n",
        "        \"\"\"\r\n",
        "        category_vocab = Vocabulary()\r\n",
        "        for category in sorted(set(news_df.category)):\r\n",
        "            category_vocab.add_token(category)\r\n",
        "\r\n",
        "        word_counts = Counter()\r\n",
        "        for title in news_df.title:\r\n",
        "            for token in title.split(\" \"):\r\n",
        "              if token not in string.punctuation:\r\n",
        "                  word_counts[token] += 1\r\n",
        "        \r\n",
        "        title_vocab = SequenceVocabulary()\r\n",
        "        for word, word_count in word_counts.itmes():\r\n",
        "            if word_count >= cutoff:\r\n",
        "                title_vocab.add_token(word)\r\n",
        "\r\n",
        "        return cls(title_vocab, category_vocab)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGGRIPP_Z_3"
      },
      "source": [
        "### The NewsClassifier Model\r\n",
        "\r\n",
        "To intialize an embedding matrix with a pretrained embedding, the pretrained embedding is loaded from disk a subset of embedding vectors are selected based on words that are present in the data used, and then finally setting the `Embedding` layer's weight matrix to the loaded subset. Beyond only selecting word vectors for those words present in that data, it could occur there are no word vectors available. One approach to handle this situation is to initialize word vectors with the Xavier Uniform method. \r\n",
        "\r\n",
        "*Example 5-13. Selecting a subset of the word embeddings based on the vocabulary*\r\n",
        "\r\n",
        "```\r\n",
        "def load_glove_from_file(glove_filepath):\r\n",
        "    \"\"\"Load the GloVe embeddings\r\n",
        "\r\n",
        "    Args:\r\n",
        "        glove_filepath(str): path to the glove embeddings file\r\n",
        "    Returns:\r\n",
        "        word_to_index (dict), embeddings (numpy.ndarray)\r\n",
        "    \"\"\"\r\n",
        "    word_to_index = []\r\n",
        "    embeddings = []\r\n",
        "    with open(glove_filepath, \"r\") as fp:\r\n",
        "        line = line.split(\" \") # each line: word num1 num2 ...\r\n",
        "        word_to_index[line[0]] = index # word = line[0]\r\n",
        "        embedding_i = np.array([float(val) for val in line[1:]])\r\n",
        "        embeddings.append(embedding_i)\r\n",
        "    \r\n",
        "    return word_to_index, np.stack(embeddings)\r\n",
        "\r\n",
        "def make_embedding_matrix(glove_filepath, words):\r\n",
        "    \"\"\"Create embedding matrix for a specific set of words.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        glove_filepath (str): file path to the glove embeddings\r\n",
        "        words (list): list of words in the dataset\r\n",
        "    Returns:\r\n",
        "        final_embeddings (numpy.ndarray): embedding matrix\r\n",
        "    \"\"\"\r\n",
        "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\r\n",
        "    embedding_size = glove_embeddings.shape[1]\r\n",
        "    final_embeddings = np.zeros((len(words), embedding_size))\r\n",
        "\r\n",
        "    for i, word in enumerate(words):\r\n",
        "        if word in word_to_idx:\r\n",
        "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\r\n",
        "        else:\r\n",
        "            embedding_i = torch.ones(1, embedding_size)\r\n",
        "            torch.nn.init.xavier_uniform(embedding_i)\r\n",
        "            final_embeddings[i, :] = embedding_i\r\n",
        "    \r\n",
        "    return final_embeddings\r\n",
        "```\r\n",
        "\r\n",
        "The `NewsClassifier` uses and `Embedding` layer to map input token indices to a vector representation. The weight matrix of this layer is replaced with the pretrained embeddings. The embedding is then used in the `forward()` method to map from the indices to the vectors, which is then fed into a sequence of convolution layers.\r\n",
        "\r\n",
        "*Example 5-14. Implementing the NewsClassifier*\r\n",
        "\r\n",
        "```\r\n",
        "class NewsClassifier(nn.Module):\r\n",
        "    def __init__(self, embedding_size, num_embeddings, num_channels,\r\n",
        "                 hidden_dim, num_classes, dropout_p,\r\n",
        "                 pretrained_embeddings=None, padding_idx=0):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            embedding_size (int): the size of the embedding vectors\r\n",
        "            num_embeddings (int): number of embedding vectors\r\n",
        "            filter_width (int): width of the convolutional kernels\r\n",
        "            num_channels (int): number of convolutional kernels per layer\r\n",
        "            hidden_dim (int): size of the hidden dimension\r\n",
        "            num_classes (int): number of classes of classes in the classification\r\n",
        "            dropout_p (float): a dropout parameter\r\n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\r\n",
        "                default is None.\r\n",
        "            padding_idx (int): an index representing a null position\r\n",
        "        \"\"\"\r\n",
        "        super(NewsClassifier, self).__init__()\r\n",
        "\r\n",
        "        if pretrained_embeddings is None:\r\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\r\n",
        "                                    num_embeddings=num_embeddings,\r\n",
        "                                    padding_idx=padding_idx)\r\n",
        "\r\n",
        "        else:\r\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\r\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\r\n",
        "                                    num_embeddings=num_embeddings,\r\n",
        "                                    padding_idx=padding_idx,\r\n",
        "                                    _weight=pretrained_embeddings)\r\n",
        "\r\n",
        "        self.convnet = nn.Sequential(\r\n",
        "            nn.Conv1d(in_channels=embedding_size,\r\n",
        "                   out_channels=num_channels, kernel_size=3),\r\n",
        "            nn.ELU(),\r\n",
        "            nn.Conv1d(in_channels=num_channels,\r\n",
        "                   out_channels=num_channels, kernel_size=3,\r\n",
        "                   stride=2),\r\n",
        "            nn.ELU(),\r\n",
        "            nn.Conv1d(in_channels=num_channels,\r\n",
        "                   out_channels=num_channels, kernel_size=3,\r\n",
        "                   stride=2),\r\n",
        "            nn.ELU(),\r\n",
        "            nn.Conv1d(in_channels=num_channels,\r\n",
        "                   out_channels=num_channels, kernel_size=3),\r\n",
        "            nn.ELU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self._dropout_p = dropout_p\r\n",
        "        self.fc1 = nn.Linear(num_channels, hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x_in, apply_softmax=False):\r\n",
        "        \"\"\"The forward pass of the classifier\r\n",
        "\r\n",
        "        Args:\r\n",
        "            x_in (torch.Tensor): an input data tensor\r\n",
        "                x_in.shape should be (batch, dataset._max_seq_length)\r\n",
        "            apply_softmax (bool): a flag for the softmax activation\r\n",
        "                should be false if used with the cross-entropy losses\r\n",
        "        Returns:\r\n",
        "            the resulting tensor\r\n",
        "                tensor.shape should be (batch, num_classes)\r\n",
        "        \"\"\"\r\n",
        "        # embed and permute so features are channels\r\n",
        "        x_embedded = self.emb(x_in).permute(0, 2, 1)\r\n",
        "        features = self.convnet(x_embedded)\r\n",
        "\r\n",
        "        # average and remove the extra dimension\r\n",
        "        remaing_size = features.size(dim=2)\r\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\r\n",
        "        features = F.dropout(features, p=self._dropout_p)\r\n",
        "\r\n",
        "        # final linear layer to produce classification outputs\r\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features),\r\n",
        "                                               p=self._dropout_p))\r\n",
        "        prediction_vector = self.fc2(intermediate_vector)\r\n",
        "\r\n",
        "        if apply_softmax:\r\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\r\n",
        "\r\n",
        "        return prediction_vector\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxRd0IAALMM"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "The training routine follows the same steps as previously described.\r\n",
        "\r\n",
        "*Example 5-15. Arguments to the CNN NewsClassifier using pretrained embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "args = Namespace(\r\n",
        "    # Data and path hyperparameters\r\n",
        "    news_csv=\"data/ag_news/news_with_splits.csv\",\r\n",
        "    vectorizer_file=\"vectorizer.json\",\r\n",
        "    model_state_file=\"model.pth\",\r\n",
        "    save_dir=\"model_storage/ch5/document_classification\",\r\n",
        "    # Model hyperparamters\r\n",
        "    glove_filepath='data/glove/glove.6B.100d.txt',\r\n",
        "    use_glove=False,\r\n",
        "    embedding_size=100,\r\n",
        "    hidden_dim=100,\r\n",
        "    num_channels=100,\r\n",
        "    # Training hyperparameter\r\n",
        "    seed=1337,\r\n",
        "    learning_rate=0.001,\r\n",
        "    dropout_p=0.1,\r\n",
        "    batch_size=128,\r\n",
        "    num_epochs=100,\r\n",
        "    early_stopping_criteria=5,\r\n",
        "    # Runtime options omitted\r\n",
        ")\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAdlrfokAT8P"
      },
      "source": [
        "### Model Evaluation and Prediction\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2ZVbJSAXP-"
      },
      "source": [
        "#### Evaluating on the test dataset\r\n",
        "\r\n",
        "Evaluating the model's performance on the test set follows the same procedure from previous examples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT10jwLDAZsh"
      },
      "source": [
        "#### Predicting the category of novel news headlines\r\n",
        "\r\n",
        "To perform inference on a single news headline, the text has to be preprocessed in the same manner as the training data. The same pipeline for going from processing to vectorization is used.\r\n",
        "\r\n",
        "*Example 5-16. Predicting with the trained model*\r\n",
        "\r\n",
        "```\r\n",
        "def predict_category(title, classifier, vectorizer, max_length):\r\n",
        "    \"\"\"Predict a news category for a new title\r\n",
        "\r\n",
        "    Args:\r\n",
        "        title (str): a raw title string\r\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\r\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\r\n",
        "        max_length (int): the max sequence length\r\n",
        "            Note: CNNs are sensitive to the input data tensor size.\r\n",
        "              This ensures it's kept to the same size as the training data.\r\n",
        "        \"\"\"\r\n",
        "        title = preprocess_text(title)\r\n",
        "        vectorized_title = \\\r\n",
        "            torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\r\n",
        "        result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\r\n",
        "        probability_values, indices = result.max(dim=1)\r\n",
        "        predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\r\n",
        "\r\n",
        "        return {'category': predicted_category,\r\n",
        "                'probability': probability_values.item()}\r\n",
        "```"
      ]
    }
  ]
}