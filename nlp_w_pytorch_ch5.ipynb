{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjWE0Rofhqj3ytSqG1U+8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdRDG3KL5S3a"
      },
      "source": [
        "# Embedding Words and Types\r\n",
        "\r\n",
        "Part of implementing any NLP task involves dealing with different kinds of discrete types. Examples of discrete types are:\r\n",
        "\r\n",
        "* words\r\n",
        "* characters\r\n",
        "* parts-of-speech tags (POS)\r\n",
        "* named entities\r\n",
        "* named entity types\r\n",
        "* parse features \r\n",
        "* items in a product catalog\r\n",
        "\r\n",
        "Any input feature that comes from a finite (or countably finite) set (aka a vocabulary), it is a *discrete type*.\r\n",
        "\r\n",
        "One of the core successes to deep learning in NLP is the method of representing discrete types as dense vectors. \"Representation learning\" or \"embedding\" refer to learning a mapping from one discrete type to a point in a vector space. In the context of words, this mapping is referred to as a *word embedding*. Other embedding methods exist, such as count-based embeddings (TF-IDF). The focus here will be *learning-based* or *prediction-based* embedding methods, where the representations are learned by maximizing an objective for a specific learning task. One such example is predicting a word based on context. These learned embeddings are so quintessential to modern NLP that it can be expected the performance on any NLP task will improve by adding one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu00Qwau5_f3"
      },
      "source": [
        "## Why Learn Embeddings?\r\n",
        "\r\n",
        "Learned embeddings have several advantages over more classical representations, such as count-based methods that are heuristically constructed.\r\n",
        "\r\n",
        "First, they are more computationally efficient since their size does not scale with the size of the vocabularly. Second, count-based methods result in high-dimensional vectors that encode redundant information along many dimensions. Third, very high dimensions lead to problems in fitting machine learning models (*the curse of dimensionality*). Finally, learned representations are more suited to the task at hand, whereas count-based or low dimensional approaches (SVD and PCA) are not necessarily optimized for the relevant task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlELToI6Ciy"
      },
      "source": [
        "### Efficiency of Embeddings\r\n",
        "\r\n",
        "One of the major efficiencies of word embeddings is their size is typically much smaller than those of one-hot or count-based representations. Typical sizes range between 25 and 500 dimensions, usually dicatated by hardware limitations.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHfkXR16Hk7"
      },
      "source": [
        "### Approaches to Learning Word Embeddings\r\n",
        "\r\n",
        "All word embedding methods train with just words in a supervised fashion. This is accomplished by constructing *auxiliary* tasks in which the data is implicitly labeled. Some examples:\r\n",
        "\r\n",
        "* given a sequence of words, predict the new word (also called the *langauge modeling* task)\r\n",
        "* given a sequence of words before and after, predict the missing word\r\n",
        "* give a word, predict words that occur within a window, indepdent of the position\r\n",
        "\r\n",
        "Generally, it's more worthwhile to use a pretrained word embedding and fine-tune than to create one from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G93wPQaf6LjI"
      },
      "source": [
        "### The Practical Use of Pretrained Word Embeddings\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRo30C146Q85"
      },
      "source": [
        "#### Loading Embeddings\r\n",
        "\r\n",
        "Some popular pretrained word embeddings are:\r\n",
        "\r\n",
        "* Word2Vec \r\n",
        "* GLoVe\r\n",
        "* FastText\r\n",
        "\r\n",
        "The typical file format for these embeddings is as follows: each line starts with the word/type that is being embedded and is followed by a sequence of numbers (the vector representation). The length of this sequence is the dimension of the representation (embedding dimension). \r\n",
        "\r\n",
        "A utility class called `PreTrainedWordEmbeddings` is used to load and process embeddings. This class builds an in-memory index of all the word vectors for quick lookups and nearest-neighbor queries using an approximate nearest-neighbor package called `annoy`.\r\n",
        "\r\n",
        "*Example 5-1. Using pretrained word embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m11boiTara_F",
        "outputId": "666a794e-edb5-4d74-a742-8341e6f24645"
      },
      "source": [
        "%%shell\r\n",
        "# download glove model \r\n",
        "wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "unzip glove*.zip\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 20:12:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-01-31 20:12:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-01-31 20:12:31--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.00MB/s    in 6m 34s  \n",
            "\n",
            "2021-01-31 20:19:05 (2.09 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQVBs-g45PPX"
      },
      "source": [
        "# !pip install annoy\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from annoy import AnnoyIndex\r\n",
        "\r\n",
        "class PretrainedEmbeddings(object):\r\n",
        "    def __init__(self, word_to_index, word_vectors):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word_to_index (dict): mapping from word to integers\r\n",
        "            word_vectors (list of numpy arrays)\r\n",
        "        \"\"\"\r\n",
        "        self.word_to_index = word_to_index\r\n",
        "        self.word_vectors = word_vectors\r\n",
        "        self.index_to_word = \\\r\n",
        "            {v: k for k, v in self.word_to_index.items()}\r\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]),\r\n",
        "                                metric='euclidean')\r\n",
        "        for _, i in self.word_to_index.items():\r\n",
        "            self.index.add_item(i, self.word_vectors[i])\r\n",
        "        self.index.build(50)\r\n",
        "\r\n",
        "    @classmethod \r\n",
        "    def from_embeddings_file(cls, embedding_file):\r\n",
        "        \"\"\"Instantiate from pretrained vector file.\r\n",
        "\r\n",
        "        Vector file should be the format:\r\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\r\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\r\n",
        "\r\n",
        "        Args:\r\n",
        "            embedding_file (str): location of the file\r\n",
        "        Returns:\r\n",
        "            instance of PreTrainedEmbeddings\r\n",
        "        \"\"\"\r\n",
        "        word_to_index = {}\r\n",
        "        word_vectors = []\r\n",
        "        with open(embedding_file) as fp:\r\n",
        "            for line in fp.readlines():\r\n",
        "                line = line.split(\" \")\r\n",
        "                word = line[0]\r\n",
        "                vec = np.array([float(x) for x in line[1:]])\r\n",
        "\r\n",
        "                word_to_index[word] = len(word_to_index)\r\n",
        "                word_vectors.append(vec)\r\n",
        "        return cls(word_to_index, word_vectors)\r\n",
        "\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60WgtNvHFAS"
      },
      "source": [
        "# load glove embeddings\r\n",
        "embeddings = \\\r\n",
        "    PretrainedEmbeddings.from_embeddings_file('glove.6B.100d.txt')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LK-w2CHh4d",
        "outputId": "d04e4d9c-61a1-45af-f896-d0d079a7403a"
      },
      "source": [
        "# print the index of the word 'working'\r\n",
        "print(embeddings.word_to_index['working'])\r\n",
        "# print the word with index 500\r\n",
        "print(embeddings.index_to_word[500])\r\n",
        "# print the word embedding of the word 'working'\r\n",
        "embeddings.word_vectors[500]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "working\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.076552 ,  0.17843  , -0.44464  ,  0.085718 ,  0.28268  ,\n",
              "       -0.30546  , -0.30637  ,  0.36632  , -0.19919  ,  0.35636  ,\n",
              "        0.088981 , -0.7717   ,  0.68709  , -0.055057 , -0.47002  ,\n",
              "       -0.52158  ,  0.58331  , -0.32255  , -0.28368  , -0.020115 ,\n",
              "        0.12133  ,  0.63264  ,  0.2717   , -0.61169  , -0.015634 ,\n",
              "       -0.54613  , -0.19113  , -0.77745  , -0.048714 ,  0.38825  ,\n",
              "       -0.68519  ,  0.71731  , -0.075302 , -0.26239  , -0.013498 ,\n",
              "        0.19442  , -0.19793  ,  0.040908 ,  0.78602  , -0.049446 ,\n",
              "       -0.83782  ,  0.10923  , -0.15471  ,  0.12925  , -0.26784  ,\n",
              "        0.045059 , -0.60726  , -0.41779  , -0.063718 , -0.58079  ,\n",
              "       -0.40284  , -0.37669  , -0.18443  ,  1.4475   , -0.15176  ,\n",
              "       -2.215    , -0.22298  , -0.28886  ,  1.3392   ,  0.55239  ,\n",
              "        0.022604 ,  0.70506  , -0.34004  , -0.26593  ,  0.80853  ,\n",
              "        0.26161  ,  0.38258  ,  0.44347  ,  0.37905  ,  0.26225  ,\n",
              "        0.082587 , -0.049931 , -0.19572  , -0.48894  ,  0.20751  ,\n",
              "        0.1884   ,  0.061963 , -0.23657  , -0.48063  , -0.16153  ,\n",
              "        0.49952  ,  0.4713   , -0.20645  ,  0.30818  , -1.7238   ,\n",
              "        0.46214  ,  0.27348  ,  0.15508  , -1.1395   , -0.68702  ,\n",
              "        0.24433  , -0.37973  ,  0.20316  , -0.25226  , -0.30063  ,\n",
              "        0.090957 , -0.21252  ,  0.0059495,  0.6699   , -0.070276 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzl7SGGv7oiv"
      },
      "source": [
        "#### Relationships between word embeddings\r\n",
        "\r\n",
        "One of the features of word embeddings is the encoded syntatic and semantic relationships between various words. For words such as cats and dogs, their respective word vectors are close together relative to other words, such as ducks or elephants.\r\n",
        "\r\n",
        "One way to explore these similarities is the analogy task: $$\\text{Word1 : Word2 :: Word3 : ____}$$\r\n",
        "\r\n",
        "In this task, three words are provided with a missing fourth word. The fourth word is chosen such that it's relationship with the third word is congruent to the relationship between words one and two. With word embeddings, these relationships are encoded spatially. Subtracting the word embedding of `Word2` from `Word1` yields a difference vector that represents this relationship. By adding `Word3` to this difference vector, a new vector is produced that is close to the fourth word. The other word embeddings can be queried using nearest-neighors to find the word embedding most similar to this output vector, solving the analogy task.\r\n",
        "\r\n",
        "*Example 5-2. The analogy task using word embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "    def get_embedding(self, word):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            word (str)\r\n",
        "        Returns:\r\n",
        "            an embedding (numpy.ndarray)\r\n",
        "        \"\"\"\r\n",
        "        return self.word_vectors[self.word_to_index[word]]\r\n",
        "    \r\n",
        "    def get_closest_to_vector(self, vector, n=1):\r\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\r\n",
        "        Args:\r\n",
        "            vector (np.ndarray): should match the size of the vectors in the Annoy index\r\n",
        "            n (int): the number of neighbors to return\r\n",
        "        Returns:\r\n",
        "            [str, str, ...]: words nearest to the given vector\r\n",
        "                The words are not ordered by distance\r\n",
        "        \"\"\"\r\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\r\n",
        "        return [self.index_to_word[neighbor]\r\n",
        "                    for neighbor in nn_indices]\r\n",
        "    \r\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\r\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\r\n",
        "\r\n",
        "        Analogies are word1 is to word2 as word3 is to __\r\n",
        "        This method will print: word1 : word2 :: word3 : word4\r\n",
        "\r\n",
        "        Args:\r\n",
        "            word1 (str)\r\n",
        "            word2 (str)\r\n",
        "            word3 (str)\r\n",
        "        \"\"\"\r\n",
        "        vec1 = self.get_embedding(word1)\r\n",
        "        vec2 = self.get_embedding(word2)\r\n",
        "        vec3 = self.get_embedding(word3)\r\n",
        "\r\n",
        "        # Simple hypothesis: Analogy is spatial relationship\r\n",
        "        spatial_relationship = vec2 - vec1\r\n",
        "        vec4 = vec3 + spatial_relationship\r\n",
        "\r\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\r\n",
        "        existing_words = set([word1, word2, word3])\r\n",
        "        closest_words = [word for word in closest_words \r\n",
        "                                if word not in existing_words]\r\n",
        "\r\n",
        "        if len(closest_words) == 0:\r\n",
        "            print(\"Could not find nearest neighbors for the vector!\")\r\n",
        "            return\r\n",
        "        \r\n",
        "        for word4 in closest_words:\r\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4) )\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLPW7un_771z"
      },
      "source": [
        "*Example 5-3. Word embeddings encode many linguistics relationships, as illustrated using the SAT analogy task*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y9WJurB8DdX",
        "outputId": "ac56e483-c677-4c48-d774-d60347bd2866"
      },
      "source": [
        "# Relationship 1: the relationship between gendered nouns and pronouns\r\n",
        "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man : he :: woman : she\n",
            "man : he :: woman : her\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur274pQwTp_S",
        "outputId": "33503de6-c4c5-4d6a-a240-78060d9443a9"
      },
      "source": [
        "# Relationship 2: Verb-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly : plane :: sail : ship\n",
            "fly : plane :: sail : vessel\n",
            "fly : plane :: sail : boat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9USYUFWcT6dv",
        "outputId": "39b939f2-e9e2-45d7-9f8c-dc35e01a18e7"
      },
      "source": [
        "# Relationship 3: Noun-noun relationships\r\n",
        "embeddings.compute_and_print_analogy('cat', 'kitten', 'dog')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat : kitten :: dog : puppy\n",
            "cat : kitten :: dog : puppies\n",
            "cat : kitten :: dog : junkyard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRtOACU8HsB"
      },
      "source": [
        "*5-4. An example illustrating the danger of using cooccurrences to encode meaning---sometimes they do not!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Ufl4Ag8Tbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-q3MKxT8dsx"
      },
      "source": [
        "*Example 5-5. Watch out for protected attributes such as gender encoded in word embeddings. This can introduce unwanted biases in downstream models.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tynKBZs68mGb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdtzusZI8q4w"
      },
      "source": [
        "*Example 5-6. Cultural gender bias encoded in vector analogy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbf8daZu8v2_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHRwKBAa8zfa"
      },
      "source": [
        "## Exampled: Learning the Continuous Bag of Words Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHNOrJn836G"
      },
      "source": [
        "### The Frankenstein Dataset\r\n",
        "\r\n",
        "*Example 5-7. Constructing a dataset class for the CBOW task*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROVOvUG9FmY"
      },
      "source": [
        "### Vocabularly, Vectorizer, DataLoader\r\n",
        "\r\n",
        "*Exampled 5-8. A Vectorizer for the CBOW data*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWlBD2k9Q0H"
      },
      "source": [
        "### The CBOWClassifier Model\r\n",
        "\r\n",
        "*Example 5-9. The CBOWClassifier model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITb29y39X_N"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-10. Arguments to the CBOW training script*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvFE6dvk-EeD"
      },
      "source": [
        "### Model Evaluation and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk-tU26U-HWe"
      },
      "source": [
        "## Example: Transfer Learning Using Pretrained Embeddings for Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQQgJwJ-pAm"
      },
      "source": [
        "### The AG News Dataset\r\n",
        "\r\n",
        "*Example 5-11. The NewsDataset.__getitem__() method*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAPLV_N9_PyE"
      },
      "source": [
        "### Vocabulary, Vectorizer, and DataLoader\r\n",
        "\r\n",
        "*Exampled 5-12. Implementing a Vectorizer for the AG News dataset*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGGRIPP_Z_3"
      },
      "source": [
        "### The NewsClassifier Model\r\n",
        "\r\n",
        "*Example 5-13. Selecting a subset of the word embeddings based on the vocabulary*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n",
        "\r\n",
        "*Example 5-14. Implementing the NewsClassifier*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxRd0IAALMM"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-15. Arguments to the CNN NewsClassifier using pretrained embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAdlrfokAT8P"
      },
      "source": [
        "### Model Evaluation and Prediction\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2ZVbJSAXP-"
      },
      "source": [
        "#### Evaluating on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT10jwLDAZsh"
      },
      "source": [
        "#### Predicting the category of novel news headlines\r\n",
        "\r\n",
        "*Example 5-16. Predicting with the trained model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    }
  ]
}