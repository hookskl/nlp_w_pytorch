{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMu7YRnusw9FdwhYNHhpSWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdRDG3KL5S3a"
      },
      "source": [
        "# Embedding Words and Types\r\n",
        "\r\n",
        "Part of implementing any NLP task involves dealing with different kinds of discrete types. Examples of discrete types are:\r\n",
        "\r\n",
        "* words\r\n",
        "* characters\r\n",
        "* parts-of-speech tags (POS)\r\n",
        "* named entities\r\n",
        "* named entity types\r\n",
        "* parse features \r\n",
        "* items in a product catalog\r\n",
        "\r\n",
        "Any input feature that comes from a finite (or countably finite) set (aka a vocabulary), it is a *discrete type*.\r\n",
        "\r\n",
        "One of the core successes to deep learning in NLP is the method of representing discrete types as dense vectors. \"Representation learning\" or \"embedding\" refer to learning a mapping from one discrete type to a point in a vector space. In the context of words, this mapping is referred to as a *word embedding*. Other embedding methods exist, such as count-based embeddings (TF-IDF). The focus here will be *learning-based* or *prediction-based* embedding methods, where the representations are learned by maximizing an objective for a specific learning task. One such example is predicting a word based on context. These learned embeddings are so quintessential to modern NLP that it can be expected the performance on any NLP task will improve by adding one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu00Qwau5_f3"
      },
      "source": [
        "## Why Learn Embeddings?\r\n",
        "\r\n",
        "Learned embeddings have several advantages over more classical representations, such as count-based methods that are heuristically constructed.\r\n",
        "\r\n",
        "First, they are more computationally efficient since their size does not scale with the size of the vocabularly. Second, count-based methods result in high-dimensional vectors that encode redundant information along many dimensions. Third, very high dimensions lead to problems in fitting machine learning models (*the curse of dimensionality*). Finally, learned representations are more suited to the task at hand, whereas count-based or low dimensional approaches (SVD and PCA) are not necessarily optimized for the relevant task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlELToI6Ciy"
      },
      "source": [
        "### Efficiency of Embeddings\r\n",
        "\r\n",
        "One of the major efficiencies of word embeddings is their size is typically much smaller than those of one-hot or count-based representations. Typical sizes range between 25 and 500 dimensions, usually dicatated by hardware limitations.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHfkXR16Hk7"
      },
      "source": [
        "### Approaches to Learning Word Embeddings\r\n",
        "\r\n",
        "All word embedding methods train with just words in a supervised fashion. This is accomplished by constructing *auxiliary* tasks in which the data is implicitly labeled. Some examples:\r\n",
        "\r\n",
        "* given a sequence of words, predict the new word (also called the *langauge modeling* task)\r\n",
        "* given a sequence of words before and after, predict the missing word\r\n",
        "* give a word, predict words that occur within a window, indepdent of the position\r\n",
        "\r\n",
        "Generally, it's more worthwhile to use a pretrained word embedding and fine-tune than to create one from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G93wPQaf6LjI"
      },
      "source": [
        "### The Practical Use of Pretrained Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRo30C146Q85"
      },
      "source": [
        "#### Loading Embeddings\r\n",
        "\r\n",
        "*Example 5-1. Using pretrained word embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQVBs-g45PPX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzl7SGGv7oiv"
      },
      "source": [
        "#### Relationships between word embeddings\r\n",
        "\r\n",
        "*Example 5-2. The analogy task using word embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYH-xFL-74Uk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLPW7un_771z"
      },
      "source": [
        "*Example 5-3. Word embeddings encode many linguistics relationships, as illustrated using the SAT analogy task*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y9WJurB8DdX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRtOACU8HsB"
      },
      "source": [
        "*5-4. An example illustrating the danger of using cooccurrences to encode meaning---sometimes they do not!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Ufl4Ag8Tbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-q3MKxT8dsx"
      },
      "source": [
        "*Example 5-5. Watch out for protected attributes such as gender encoded in word embeddings. This can introduce unwanted biases in downstream models.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tynKBZs68mGb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdtzusZI8q4w"
      },
      "source": [
        "*Example 5-6. Cultural gender bias encoded in vector analogy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbf8daZu8v2_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHRwKBAa8zfa"
      },
      "source": [
        "## Exampled: Learning the Continuous Bag of Words Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHNOrJn836G"
      },
      "source": [
        "### The Frankenstein Dataset\r\n",
        "\r\n",
        "*Example 5-7. Constructing a dataset class for the CBOW task*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROVOvUG9FmY"
      },
      "source": [
        "### Vocabularly, Vectorizer, DataLoader\r\n",
        "\r\n",
        "*Exampled 5-8. A Vectorizer for the CBOW data*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWlBD2k9Q0H"
      },
      "source": [
        "### The CBOWClassifier Model\r\n",
        "\r\n",
        "*Example 5-9. The CBOWClassifier model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITb29y39X_N"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-10. Arguments to the CBOW training script*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvFE6dvk-EeD"
      },
      "source": [
        "### Model Evaluation and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk-tU26U-HWe"
      },
      "source": [
        "## Example: Transfer Learning Using Pretrained Embeddings for Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQQgJwJ-pAm"
      },
      "source": [
        "### The AG News Dataset\r\n",
        "\r\n",
        "*Example 5-11. The NewsDataset.__getitem__() method*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAPLV_N9_PyE"
      },
      "source": [
        "### Vocabulary, Vectorizer, and DataLoader\r\n",
        "\r\n",
        "*Exampled 5-12. Implementing a Vectorizer for the AG News dataset*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGGRIPP_Z_3"
      },
      "source": [
        "### The NewsClassifier Model\r\n",
        "\r\n",
        "*Example 5-13. Selecting a subset of the word embeddings based on the vocabulary*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n",
        "\r\n",
        "*Example 5-14. Implementing the NewsClassifier*\r\n",
        "\r\n",
        "```\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxRd0IAALMM"
      },
      "source": [
        "### The Training Routine\r\n",
        "\r\n",
        "*Example 5-15. Arguments to the CNN NewsClassifier using pretrained embeddings*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAdlrfokAT8P"
      },
      "source": [
        "### Model Evaluation and Prediction\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2ZVbJSAXP-"
      },
      "source": [
        "#### Evaluating on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT10jwLDAZsh"
      },
      "source": [
        "#### Predicting the category of novel news headlines\r\n",
        "\r\n",
        "*Example 5-16. Predicting with the trained model*\r\n",
        "\r\n",
        "```\r\n",
        "```"
      ]
    }
  ]
}