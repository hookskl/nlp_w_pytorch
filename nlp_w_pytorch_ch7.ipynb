{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWqzMG3Jcnndbj1SMY8p9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6W6cw-7MGTw"
      },
      "source": [
        "# Intermediate Sequence Modeling for NLP\r\n",
        "\r\n",
        "Sequence prediction tasks seek to label each item of a sequence. Examples include:\r\n",
        " \r\n",
        " * *language modeling*: predict the next word given a sequence of words at each step\r\n",
        " * *parts-of-speech tagging*: predict the grammatical part of speech for each word\r\n",
        " * *named entity recognition*: predict whether each word is part of a named enity (`Person`, `Location`, `Product`, `Organization`, etc.)\r\n",
        "\r\n",
        "Sequence prediction is also sometimes referred to as *sequence labeling*.\r\n",
        "\r\n",
        "Elman RNNs in practice fail to capture long-range dependencies necessary to perform well on tasks such as sequence prediction. Gated RNNs use a different architecture that helps circumvent this shortcoming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-7db3cMSn0"
      },
      "source": [
        "## The Problem with Vanilla RNNs (Elman RNNs)\r\n",
        "\r\n",
        "The Elman RNN has two issues that make it unsuitable for many tasks:\r\n",
        "\r\n",
        "1. it is unable to retain information for long sequences\r\n",
        "2. numerical stability of its gradients\r\n",
        "\r\n",
        "The first issue stems from the fact that the Elman RNN merely updates its hidden state vector at each time step, regardless of whether the update made sense or not. The RNN has no control of what values are retained versus which ones are discarded. A more desirable behaviour would be to let the RNN decide when to update and by how much or what parts to update.\r\n",
        "\r\n",
        "Second, for really long sequences, Vanilla RNNs tend to suffer from numerical stable gradients. In these cases, the gradients can shrink towards `0` (*vanishing gradients*) or grow to infinity (*exploding gradients*). Either case has severe consequences on the model's ability to train. Techiniques exist for dealing with these gradient issues (ReLU, gradient clipping, etc.) but fail to improve the model better than using *gating*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9fBEOYXMYPT"
      },
      "source": [
        "### Gating as a Solution to a Vanilla RNN's Challenges\r\n",
        "\r\n",
        "To gain some intuition about gating, consider adding two values, $a$ and $b$, with the optional constraint to limit how much of $b$ is added. This can be written as : $$a + \\lambda b \\text{, where } 0 \\leq \\lambda \\leq 1.$$\r\n",
        "\r\n",
        "Here $\\lambda$ acts as a \"switch\" or \"gate\", controlling how much $b$ contributes to the total sum. This is the basic idea behind gating in RNNs. To see how this is incorporated, recall how the Elman RNN updates the hidden state vector: $$h_t=h_{t-1}+F(h_{t-1}, x_t) \\text{, where}$$ $$h_t \\text{ is the hidden state vector at some time step  } t,$$ $$x_t \\text{ is the input at some time step  } t, \\text{ and}$$ $$F \\text{ is the recurrent computation of the RNN}$$\r\n",
        "\r\n",
        "Modifying this equation with the ideas above, yields: $$h_t=h_{t-1}+\\lambda(h_{t-1}, x_t)F(h_{t-1}, x_t)$$\r\n",
        "\r\n",
        "but now instead of being a constant, $\\lambda$ is function of the previous hidden state vector and current data input and still maps values to $[0,1]$. The function $\\lambda$ controls how much of the current input gets to update the hidden state $h_{t-1}$, as well as being context dependent. This is the basic idea behind all gated RNNs. \r\n",
        "\r\n",
        "*Long short-term memory* networks (LSTMs) are a flavor of gated RNNs that extends this idea further, where not only are the updates controlled, but also intentional forgetting of values from the previous hidden state. \r\n",
        "\r\n",
        "Another variant is the *gated recurrent unit* (GRU). Both are easily implemented in PyTorch, replacing the `nn.RNN` or `nn.RNNCell` with `nn.LSTM` or `nn.LSTMCell`, respectively. No other code changes are required (also applies to GRUs). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrwpJiPFMlQh"
      },
      "source": [
        "## Example: A Character RNN for Generating Surnames\r\n",
        "\r\n",
        "This example uses the surnames dataset to introduce a new sequence prediction task: using an RNN to generate a new surname. What this means is at each time step the RNN is computing a probability distribution over the set of possible characters given a prior sequence of characters. These distributions can be used either for optimizing the network and improving predictions, or generating a new surname. \r\n",
        "\r\n",
        "Two models will be used for this task: an unconditional model and a conditional model. The only difference in these two models is the conditional model will start with a bias coming from an embedding of a given nationality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUsaDrtBMpON"
      },
      "source": [
        "### The SurnameDataset Class\r\n",
        "\r\n",
        "The `SurnameDataset` class remains largely unchanged from implementation for classifying a surname's nationality. However, to accomodate the difference in tasks, the `.__getitem__()` method is modified to output the sequences of integers for the prediction targets. The method references the `Vectorizer` for computing the sequence of integers that serve as the input (the `from_vector`) and the sequence of integers that serve as the output (the `to_vector`). \r\n",
        "\r\n",
        "*Example 7-1. The `SurnameDataset.__getitem__()` method for a sequence prediction task*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameDataset(Dataset):\r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\r\n",
        "        \"\"\"Load dataset and make a vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_csv (str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameDataset\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        surname_df = pd.read_csv(surname_csv)\r\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(surname_df))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point\r\n",
        "        Returns:\r\n",
        "            a dictionary holding the data point: (x_data, y_target, class_index)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "\r\n",
        "        from_vector, to_vector = \\\r\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\r\n",
        "\r\n",
        "        nationality_index = \\\r\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\r\n",
        "\r\n",
        "        return {'x_data': from_vector,\r\n",
        "                'y_target': to_vector,\r\n",
        "                'class_index': nationality_index}\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6EBgnMcNvd-"
      },
      "source": [
        "### The Vectorization Data Structures\r\n",
        "\r\n",
        "Similar to previous implementations, there are three main data structures used to transform each sequence of characters into its vectorized form:\r\n",
        "\r\n",
        "* `SequenceVocabulary` to map tokens to integers\r\n",
        "* `SurnameVectorizer` to coordinate the integer mappings\r\n",
        "* `DataLoader` to group the vectorizer's results into minibatches\r\n",
        "\r\n",
        "The `DataLoader` is unchanged from previous examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbjBOZecNyal"
      },
      "source": [
        "#### SurnameVectorizer and END-OF-SEQUENCE\r\n",
        "\r\n",
        "For sequence prediction, the training routine is written to expect two sequences of integers which represent the token observations and the token targets at each time step. Usually, the sequence trained on is also the sequence to predict. Consequently, a single sequence of tokens (surname characters) are used to construct both observations and targets by staggering the training sequence. \r\n",
        "\r\n",
        "To start, each token is mapped to its respective integer using the `SequenceVocabularly`. Next, the begin of sequence and end of sequence tokens' indexes are added (prepended or appended) to the sequence. At this point, each data point is a sequence of indices and has the same first and last index. From here the input and output sequences are created using two different slices of the given sequence. The first slice is all tokens from the sequence except the last and the second slice is all tokens except the first. \r\n",
        "\r\n",
        "Some additional implementation details: once the sequence is converted to indices and wrapped with the beginning and ending indices, the `vector_length` is tested to ensure consistent lengths prior to stacking into minibatches. After testing vector length the two slices of sequences are created, `from_vector` and `to_vector`, and are then padded with the `mask_index` to a consistent length.\r\n",
        "\r\n",
        "*Example 7-2. The code for `SurnameVectorizer.vectorize()` in a sequence prediction task*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameVectorizer(object):\r\n",
        "    \"\"\"The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\r\n",
        "\r\n",
        "    def vectorizer(self, surname, vector_length=-1):\r\n",
        "        \"\"\"Vectorizer a surname into a vector of observations and targets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname (str): the surname to be vectorized\r\n",
        "            vector_length (int): an argument for forcing the length of index vector\r\n",
        "        Returns:\r\n",
        "            a tuple: (from_vector, to_vector)\r\n",
        "                from_vector (numpy.ndarray): the observation vector\r\n",
        "                to_vector (numpy.ndarray): the target prediction vector\r\n",
        "        \"\"\"\r\n",
        "        indices = [self.char_vocab.begin_seq_index]\r\n",
        "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\r\n",
        "        indices.append(self.char_vocab.end_seq_index)\r\n",
        "\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices) -1\r\n",
        "\r\n",
        "        from_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        from_indices = indices[:-1]\r\n",
        "        from_vector[:len(from_indices)] = from_indices\r\n",
        "        from_vector[len(from_indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        to_vector = np.emtpy(vector_length, dtype=np.int64)\r\n",
        "        to_indices = indices[1:]\r\n",
        "        to_vector[:len(to_indices)] = to_indices\r\n",
        "        to_vector[len(to_indices):] = self.char_vocab.mask_index  \r\n",
        "\r\n",
        "        return from_vector, to_vector\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_dataframe(cls, surname_df):\r\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_df (pandas.DataFrame): the surname dataset\r\n",
        "        Returns:\r\n",
        "            an instance of the SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary()\r\n",
        "        nationality_vocab = Vocabulary()\r\n",
        "\r\n",
        "        for index, row in surname_df.itterrows():\r\n",
        "            for char in row.surname:\r\n",
        "                char_vocab.add_token(char)\r\n",
        "            nationality_vocab.add_token(row.nationality)\r\n",
        "\r\n",
        "        return cls(char_vocab, nationality_vocab)     \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U21nd47PqCI"
      },
      "source": [
        "### From the ElmanRNN to the GRU\r\n",
        "\r\n",
        "Switching from vanilla RNNs to a GRU model in PyTorch is extremely simple. The GRU is instantiated using `torch.nn.GRU` and the parameters are the same as those used in the vanilla RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bHsRvGePs-Z"
      },
      "source": [
        "### Model 1: The Uncondioned SurnameGenerationModel\r\n",
        "\r\n",
        "The first model is unconditioned, meaning it does not observe the nationality before generating a surname. This means the GRU does not bias its computations towards any nationality. This bias comes from how the initial hidden vector is constructed. Here, because the model is unconditioned, this initial hidden vector consists of all `0`s.\r\n",
        "\r\n",
        "In general, the model embeds character indices, computes their sequential state using a GRU, and computes the probability of token predictions using a `Linear` layer. \r\n",
        "\r\n",
        "*Example 7-3. The unconditioned surname generation model*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameGenerationModel(nn.Module):\r\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\r\n",
        "                 batch_first=True, padding_idx=0, dropout_p=0.5):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            char_embedding_size (int): the size of the character embeddings\r\n",
        "            char_vocab_size (int): the number of characters to embed\r\n",
        "            rnn_hidden_size (int): the size of the RNN's hidden state\r\n",
        "            batch_first (bool): informs whether the input tnesors will\r\n",
        "                have batch or the sequence on the 0th dimension\r\n",
        "            padding_idx (int): the index for the tensor padding;\r\n",
        "                see torch.nn.Embedding\r\n",
        "            dropout_p (float): the probability of zeroing activations using the dropout method\r\n",
        "        \"\"\"\r\n",
        "        super(SurnameGenerationModel, self).__init__()\r\n",
        "\r\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\r\n",
        "                                     embedding_dim=char_embedding_size,\r\n",
        "                                     padding_idx=padding_idx)\r\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size,\r\n",
        "                          hidden_size=rnn_hidden_size,\r\n",
        "                          batch_first=batch_first)\r\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\r\n",
        "                            out_features=char_vocab_size)\r\n",
        "        self._dropout_p = dropout_p\r\n",
        "\r\n",
        "    def forward(self, x_in, apply_softmax=False):\r\n",
        "        \"\"\"The forward pass of the model\r\n",
        "\r\n",
        "        Args:\r\n",
        "            x_in (torch.Tensor): an input data tensor\r\n",
        "                x_in.shape should be (batch, input_dim)\r\n",
        "            apply_softmax (bool): a flag for the softmax activation\r\n",
        "                should be False during training\r\n",
        "        Returns:\r\n",
        "            the resulting tensor\r\n",
        "                tensor.shape should be (batch, output_dim)\r\n",
        "        \"\"\"\r\n",
        "        x_embedded = self.char_emb(x_in)\r\n",
        "\r\n",
        "        y_out, _ = self.rnn(x_embedded)\r\n",
        "\r\n",
        "        batch_size, seq_size, feat_size = y_out.shape\r\n",
        "        y_out = y_out.continguous().view(batch_size * seq_size, feat_size)\r\n",
        "\r\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\r\n",
        "\r\n",
        "        if apply_softmax:\r\n",
        "            y_out = F.softmax(y_out, dim=1)\r\n",
        "\r\n",
        "        new_feat_size = y_out.shape[-1]\r\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\r\n",
        "\r\n",
        "        return y_out\r\n",
        "```\r\n",
        "\r\n",
        "The primary difference in this sequence classification task is how the state vectors computed by the RNN are handled. Previously, a single vector per batch index was retrieved and predictions were performed using those single vectors. However, now the 3D tensors are reshaped into 2D tensors (a matrix) such that the row dimension represents every sample (batch and sequence index). The matrix is fed into the `Linear` layer and a prediction vector is computed for every sample. The output matrix is then reshaped back into a 3D tensor. The reshaping is done purely to accomodate the linear layer requiring a matrix as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxu99D5CRpJM"
      },
      "source": [
        "### Model 2: The Conditioned SurnameGenerationModel\r\n",
        "\r\n",
        "The second model uses information about nationality when computing new surnames. To do so, the initial hidden state is parameterized by embedding each nationality as a vector the size of the hidden state. As the model adjusts its parameters, it also adjusts the values in the embedding matrix so that it baises the predictions to be more sensitive to the specific nationality and the regularities of its surnames. \r\n",
        "\r\n",
        "In implementing this model, an extra `Embedding` layer is introducted to map the nationality indices to vectors the same size as the RNN's hidden layer. In the forward function, nationality indices are passed in as the initial hidden layer of the RNN. Although simple, this has a profound effect on the model's behavior. \r\n",
        "\r\n",
        "*Example 7-4. The conditioned surname generation model*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameGenerationModel(nn.Module):\r\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\r\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\r\n",
        "        # ...\r\n",
        "        self.nation_embedding = nn.Embedding(embedding_dim=rnn_hidden_size,\r\n",
        "                                             num_embeddings=num_nationalities)\r\n",
        "\r\n",
        "    def forward(self, x_in, nationality_index, apply_softmax=False):\r\n",
        "        # ...\r\n",
        "        x_embedded = self.char_embedding(x_in)\r\n",
        "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\r\n",
        "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\r\n",
        "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\r\n",
        "        # ...                                             \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPamdbS0Sjg0"
      },
      "source": [
        "### The Training Routine and Results\r\n",
        "\r\n",
        "The training routine remains similar to previous implementations, with a few major differences.\r\n",
        "\r\n",
        "To compute the loss, 3D tensors are reshaped into 2D tensors and the masking index is coordinated so the loss function doesn't utilized masked positions in computations and updates. The first change is needed because predictions are made at each time step. Two helper functions are used to facilitate these operations. \r\n",
        "\r\n",
        "*Example 7-5. Handling three-dimensional tensors and sequence-wide loss computations*\r\n",
        "\r\n",
        "```\r\n",
        "def normalize_sizes(y_pred, y_true):\r\n",
        "    \"\"\"Normalize tensor sizes\r\n",
        "\r\n",
        "    Args:\r\n",
        "        y_pred (torch.Tensor): the output of the model\r\n",
        "            if a 3-d tensor, reshapes to a matrix\r\n",
        "        y_true (torch.Tensor): the target predictions\r\n",
        "            if a matrix, reshapes to be a vector\r\n",
        "    \"\"\"\r\n",
        "    if len(y_pred.size()) == 3:\r\n",
        "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\r\n",
        "    if len(y_true.size()) == 2:\r\n",
        "        y_true = y_true.contiguous().view(-1)\r\n",
        "    return y_pred, y_true\r\n",
        "\r\n",
        "def sequence_loss(y_pred, y_true, mask_index):\r\n",
        "    y_pred, y_true = normalize_sizes(y_pred, y_true)\r\n",
        "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\r\n",
        "```\r\n",
        "With these changes a very familiar training routine can be formulated. \r\n",
        "\r\n",
        "Of the RNN's hyperparamters, many are determined directly from teh size of the character vocabulary. This size is the number of discrete tokens that can be observed as input to the model and the number of classes in the output classification at each time step. The remaining hyperparameters are the size of the character embeddings and the size of the RNN hidden state. \r\n",
        "\r\n",
        "*Example 7-6. Hyperparameters for surname generation*\r\n",
        "\r\n",
        "```\r\n",
        "args = Namespace(\r\n",
        "    # Data and path information\r\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\r\n",
        "    vectorizer_file=\"vectorizer.json\",\r\n",
        "    model_state_file=\"model.pth\",\r\n",
        "    save_dir=\"model_storage/ch7/model1_unconditioned_surname_generation\",\r\n",
        "    # or: save_dir=\"model_storage/ch7/model2_conditioned_surname_generation\",\r\n",
        "    # Model hyperparameters\r\n",
        "    char_embedding_size=32,\r\n",
        "    rnn_hidden_size=32,\r\n",
        "    # Training hyperparameters\r\n",
        "    seed=1337,\r\n",
        "    learning_rate=0.001,\r\n",
        "    batch_size=128,\r\n",
        "    num_epochs=100,\r\n",
        "    early_stopping_criteria=5,\r\n",
        "    # Runtime options omitted\r\n",
        ")    \r\n",
        "```\r\n",
        "\r\n",
        "While per-character accuracy of the model's prediction at each time step can be measured, this task is more suited for a qualitative evaluation by direct inspection of surnames generated by the model. This is done by creating a new loop over a modified version of the `forward()` method to compute predictions at each time step. One difference is now the prediction at each time step is fed back in as the input for the next time step. Each time step results in a prediction vector that is then converted to a probability distribution using the softmax activation. Using `torch.multinomial()`, indices are sampled at a rate proportional to the probability of the index. \r\n",
        "\r\n",
        "*Example 7-7. Sampling from the unconditioned generation model*\r\n",
        "\r\n",
        "```\r\n",
        "def sample_from_model(model, vectorizer, num_samples=1, sample_size=20, \r\n",
        "                      temperature=1.0):\r\n",
        "    \"\"\"Sampel a sequence of indices from the model\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (SurnameGenerationModel): the trained model\r\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\r\n",
        "        num_samples (int): the number of samples\r\n",
        "        sample_size (int): the max length of the samples\r\n",
        "        temperature (float): accentuates or flattens the distribution\r\n",
        "            0.0 < temperature < 1.0 will make it peakier\r\n",
        "            temperature > 1.0 will make it more uniform\r\n",
        "    Returns:\r\n",
        "        indices (torch.Tensor): the matrix of indices\r\n",
        "        shape = (num_samples, sample_size)\r\n",
        "    \"\"\"\r\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \r\n",
        "                       for _ in range(num_samples)]\r\n",
        "    being_seq_index = torch.tensor(begin_seq_index,\r\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\r\n",
        "    indices = [begin_seq_index]\r\n",
        "    h_t = None\r\n",
        "\r\n",
        "    for time_step in range(sample_size):\r\n",
        "        x_t indices[time_step]\r\n",
        "        x_emb_t = model.char_emb(x_t)\r\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\r\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\r\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1))\r\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\r\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\r\n",
        "    return indices                                                                       \r\n",
        "```\r\n",
        "\r\n",
        "The sampled indices are then converted to strings for human-readable output. This is done using the `SequenceVocabulary` that was previously used to vectorize the surnames. When converting to strings, only indices up to the end of sequence index are used. With this comes the assumption the model has learned something about when surnames should end. \r\n",
        "\r\n",
        "*Example 7-8. Mapping sampled indices to surname strings*\r\n",
        "\r\n",
        "```\r\n",
        "def decode_samples(sampled_indices, vectorizer):\r\n",
        "    \"\"\"Transform indices into the string form of a surname\r\n",
        "\r\n",
        "    Args:\r\n",
        "        sampled_indices (torch.Tensor): the indices from `sample_from_model`\r\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\r\n",
        "    \"\"\"\r\n",
        "    decoded_surnames = []\r\n",
        "    vocab = vectorizer.char_vocab\r\n",
        "\r\n",
        "    for sample_index in range(sampled_indices.shape[0]):\r\n",
        "        surname = \"\"\r\n",
        "        for time_step in range(sampled_indices.shape[1]):\r\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\r\n",
        "            if sample_item == vocab.begin_seq_index:\r\n",
        "                continue\r\n",
        "            elif sample_item == vocab.end_seq_index:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                surname += vocab.lookup_index(sample_item)\r\n",
        "        decoded_surnames.append(surname)\r\n",
        "    return decoded_surnames\r\n",
        "```\r\n",
        "\r\n",
        "Using these functions, a sample of generated surnames can be inspected. \r\n",
        "\r\n",
        "*Example 7-9. Sampling from the unconditioned model*\r\n",
        "\r\n",
        "```\r\n",
        "samples = sample_from_model(unconditioned_model, vectorizer, \r\n",
        "                              num_samples=10)\r\n",
        "\r\n",
        "decode_samples(samples, vectorizer)                              \r\n",
        "```\r\n",
        "\r\n",
        "To generate samples for the conditioned model, the `sample_from_model()` function is modified to accept a list of nattionality indices rather than a specified number of samples. \r\n",
        "\r\n",
        "*Example 7-10. Sampling from a sequence model*\r\n",
        "\r\n",
        "```\r\n",
        "def sample_from_model(model, vectorizer, nationalities, sample_size=20,\r\n",
        "                      temperature=1.0):\r\n",
        "    \"\"\"Sample a sequence of indices from the model\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (SurnameGenerationModel): the trained model\r\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\r\n",
        "        nationalities (list): a list of integers representing nationalities\r\n",
        "        sample_size (int): the max length of the samples\r\n",
        "        temperature (float): accentuates or flattens the dsitribution\r\n",
        "            0.0 < temperature < 1.0 will make it peakier\r\n",
        "            temperature > 1.0 will make it more uniform\r\n",
        "    Returns:\r\n",
        "        indices (torch.Tensor): the matrix of indices\r\n",
        "        shape = (num_samples, sample_size)\r\n",
        "    \"\"\"\r\n",
        "    num_samples = len(nationalities)\r\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\r\n",
        "                       for _ in range(num_samples)]\r\n",
        "    begin_seq_index = torch.tensor(being_seq_index,\r\n",
        "                                   dtype=torch.int64).unsqeeuze(dim=1)\r\n",
        "    indices = [begin_seq_index]\r\n",
        "    nationality_indices = torch.tensor(nationalities,\r\n",
        "                                       dtype=torch.int64).unsqueeze(dim=0)\r\n",
        "    h_t = model.nation_emb(nationlity_indices)\r\n",
        "\r\n",
        "    for time_step in range(sample_size):\r\n",
        "        x_t = indices[time_step]\r\n",
        "        x_emb_t = model.char_emb(x_t)\r\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\r\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\r\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\r\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\r\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\r\n",
        "    return indices                                                                      \r\n",
        "```\r\n",
        "\r\n",
        "*Example 7-11. Sampling from the conditioned SurnameGenerationModel*\r\n",
        "\r\n",
        "```\r\n",
        "for index in range(len(vectorizer.nationality_vocab)):\r\n",
        "    nationality = vectorizer.nationality_vocab.lookup_index(index)\r\n",
        "    \r\n",
        "    print(\"Sampled for {}: \".format(nationality))\r\n",
        "\r\n",
        "    sampled_indices = sample_from_model(model=conditioned_model,\r\n",
        "                                        vectorizer=vectorizer,\r\n",
        "                                        nationalities=[index] * 3,\r\n",
        "                                        temperature=0.7)\r\n",
        "    for sampled_surname in decode_samples(sampled_indices,\r\n",
        "                                          vectorizer):\r\n",
        "        print(\"- \" + sampled_surname)                                                                                  \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nujtYL6emQ2r"
      },
      "source": [
        "## Tips and Tricks for Training Sequence Models\r\n",
        "\r\n",
        "1. *Default to gated variants over vanilla RNNs*: short of computational constraints, gated units perform better and overcome many of the numerical instability issues common to vanilla units\r\n",
        "\r\n",
        "2. *Prefer GRUs over LSTMs*: GRUs and LSTMs have comparable perfromance in practice, with GRUs utilizing far few parameters and compute resources. \r\n",
        "\r\n",
        "3. *Use Adam as the optimizer*: Adam is reliable and typically converges faster than alternatives. In cases where the model is not converging with Adam, stochastic gradient descent may help.\r\n",
        "\r\n",
        "4. *Gradient clipping*: in cases where numerical errors pop up, plotting gradients during training to understand the range and clip any outliers can ensure smoother training. Practically speaking, gradient clipping should always be used.\r\n",
        "\r\n",
        "*Example 7-12. Applying gradient clipping in PyTorch*\r\n",
        "\r\n",
        "```\r\n",
        "# define you sequence model\r\n",
        "model = ...\r\n",
        "# define loss function\r\n",
        "loss_function = ...\r\n",
        "\r\n",
        "# training loop\r\n",
        "for _ in ...\"\r\n",
        "    ...\r\n",
        "    model.zero_grad()\r\n",
        "    output, hidden = model(data, hidden)\r\n",
        "    loss = loss_function(output, targets)\r\n",
        "    loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\r\n",
        "    ...\r\n",
        "```\r\n",
        "\r\n",
        "5. *Early stopping*: Sequence models are prone to overfitting and it is generally recomended to stop training early once training error and evaluation error start to diverge. \r\n"
      ]
    }
  ]
}