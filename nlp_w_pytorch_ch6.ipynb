{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_w_pytorch_ch6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFg/14sGcQrhC2TrDkbG2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hookskl/nlp_w_pytorch/blob/main/nlp_w_pytorch_ch6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gkCJhF7K-lR"
      },
      "source": [
        "# Sequence Modeling for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_z2r_4ELGU5"
      },
      "source": [
        "## Introduction to Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUw0rzhTLN17"
      },
      "source": [
        "### Implementing an Elman RNN\r\n",
        "\r\n",
        "*Example 6-1. An implementation of the Elman RNN using Pytorch's RNNCell* \r\n",
        "\r\n",
        "```\r\n",
        "class ElmanRNN(nn.Module):\r\n",
        "    \"\"\"an Elman RNN built using RNNCell\"\"\"\r\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            input_size (int): size of the input vectors\r\n",
        "            hidden_size (int): size of the hidden state vectors\r\n",
        "            batch_first (bool): whether the 0th dimension is batch\r\n",
        "        \"\"\"\r\n",
        "        super(ElmanRNN, self).__init__()\r\n",
        "\r\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\r\n",
        "\r\n",
        "        self.batch_first = batch_first\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        def _initialize_hidden(self, batch_size):\r\n",
        "            return torch.zeros((batch_size, self.hidden_size))\r\n",
        "\r\n",
        "        def forward(self, x_in, initial_hidden=None):\r\n",
        "            \"\"\"The forward pass of the ElmanRNN\r\n",
        "\r\n",
        "            Args:\r\n",
        "                x_in (torch.Tensor): an input data tensor\r\n",
        "                    If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\r\n",
        "                    Else: x_in.shape = (seq_size, batch_size, feat_size)\r\n",
        "                initial_hidden (torch.tensor): the initial hidden sate for the RNN\r\n",
        "            Returns:\r\n",
        "                hiddens (torch.Tensor): the outputs of the RNN at each time step.\r\n",
        "                    If self.batch_first:\r\n",
        "                        hiddens.shape = (batch_size, seq_size, hidden_size)\r\n",
        "                    Else: hiddens.shape = (seq_size, batch_size, hidden_size)\r\n",
        "            \"\"\"\r\n",
        "            if self.batch_first:\r\n",
        "                batch_size, seq_size, feat_size = x_in.size()\r\n",
        "                x_in = x_in.permute(1, 0, 2)\r\n",
        "            else:\r\n",
        "                seq_size, batch_size, feat_size = x_in.size()\r\n",
        "\r\n",
        "            hiddens = []\r\n",
        "\r\n",
        "            if initial_hidden is None:\r\n",
        "                initial_hidden = self._initialize_hidden(batch_size)\r\n",
        "                initial_hidden = initial_hidden.to(x_in.device)\r\n",
        "\r\n",
        "            hidden_t = initial_hidden\r\n",
        "\r\n",
        "            for t in range(seq_size):\r\n",
        "                hidden_t = self.rnn_cell(x_in[t], hidden_t)\r\n",
        "                hiddens.append(hidden_t)\r\n",
        "\r\n",
        "            hiddens = torch.stack(hiddens)\r\n",
        "\r\n",
        "            if self.batch_first:\r\n",
        "                hiddens = hiddens.permute(1, 0, 2)\r\n",
        "\r\n",
        "            return hiddens\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi37LKbKLov7"
      },
      "source": [
        "## Example: Classifying Surname Nationality Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kusr3qLuDQ"
      },
      "source": [
        "### The SurnameDataset Class\r\n",
        "\r\n",
        "*Example 6-2. Implementing the SurnameDataset class*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameDataset(Dataset):\r\n",
        "    @classmethod\r\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\r\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_csv(str): location of the dataset\r\n",
        "        Returns:\r\n",
        "            an instance of SurnameDataset\r\n",
        "        \"\"\"\r\n",
        "        surname_df = pd.read_csv(surname_csv)\r\n",
        "        train_surname_df = surname_df[surname_df.split=='train']\r\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\r\n",
        "\r\n",
        "        Args:\r\n",
        "            index (int): the index to the data point\r\n",
        "        Returns:\r\n",
        "            a dictionary holding the data point's:\r\n",
        "                features (x_data)\r\n",
        "                label (y_target)\r\n",
        "                feature length (x_length)\r\n",
        "        \"\"\"\r\n",
        "        row = self._target_df.iloc[index]\r\n",
        "        surname_vector, vec_length = \\\r\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\r\n",
        "        \r\n",
        "        nationality_index = \\\r\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\r\n",
        "\r\n",
        "        return {'x_data': surname_vector,\r\n",
        "                'y_target': nationality_index,\r\n",
        "                'x_length': vec_length}\r\n",
        "                \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBSpyK5BL2cd"
      },
      "source": [
        "### The Vectorization Data Structures\r\n",
        "\r\n",
        "*Example 6-3. A vectorizer for surnames*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameVectorizer(object):\r\n",
        "    \"\"\"The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\r\n",
        "    def vectorize(self, surname, vector_length=-1):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            title (str): the string of characters\r\n",
        "            vector_length (int): an argument for forcing the length of index vector\r\n",
        "        \"\"\"\r\n",
        "        indices = [self.char_vocab.begin_seq_index]\r\n",
        "        indices.extend(self.char_vocab_lookup_token(token)\r\n",
        "                       for token in surname)\r\n",
        "        indices.append(self.char_vocab.end_seq_index)\r\n",
        "\r\n",
        "        if vector_length < 0:\r\n",
        "            vector_length = len(indices)\r\n",
        "\r\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\r\n",
        "        out_vector[:len(indices)] = indices\r\n",
        "        out_vector[len(indices):] = self.char_vocab.mask_index\r\n",
        "\r\n",
        "        return out_vector, len(indices)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def from_dataframe(cls, surname_df):\r\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\r\n",
        "\r\n",
        "        Args:\r\n",
        "            surname_df (pandas.DataFrame): the surnames dataset\r\n",
        "        Returns:\r\n",
        "            an instance of the SurnameVectorizer\r\n",
        "        \"\"\"\r\n",
        "        char_vocab = SequenceVocabulary()\r\n",
        "        nationality_vocab = Vocabulary()\r\n",
        "\r\n",
        "        for index, row in surname_df.itterrows():\r\n",
        "            for char in row.surname:\r\n",
        "                char_vocab.add_token(char)\r\n",
        "            nationality_vocab.add_token(row.nationality)\r\n",
        "\r\n",
        "        return cls(char_vocab, nationality_vocab)\r\n",
        "        \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR6vEZwwL-he"
      },
      "source": [
        "### The SurnameClassifier Model\r\n",
        "\r\n",
        "*Example 6-4. Implementing the SurnameClassifier model using an Elman RNN*\r\n",
        "\r\n",
        "```\r\n",
        "class SurnameClassifier(nn.Module):\r\n",
        "    \"\"\"An RNN to extract features & an MLP to classify\"\"\"\r\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\r\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            embedding_size (int): the size of the character embeddings\r\n",
        "            num_embeddings (int): the number of characters to embed\r\n",
        "            num_classes (int): the size of the prediction vector\r\n",
        "                Note: the number of nationalities\r\n",
        "            rnn_hidden_size (int): the size of the RNN's hidden state\r\n",
        "            batch_first (bool): informs whether the input tensors will \r\n",
        "                have batch or the sequence on the 0th dimension\r\n",
        "            padding_idx (int): the index for the tensor padding;\r\n",
        "                see torch.nn.Embedding\r\n",
        "        \"\"\"\r\n",
        "        super(SurnameClassifier, self).__init__()\r\n",
        "\r\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\r\n",
        "                                embedding_dim=embedding_size,\r\n",
        "                                padding_idx=padding_idx)\r\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\r\n",
        "                            hidden_size=rnn_hidden_size,\r\n",
        "                            batch_first=batch_first)\r\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\r\n",
        "                             out_features=rnn_hidden_size)\r\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\r\n",
        "                             out_features=num_classes)\r\n",
        "\r\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\r\n",
        "        \"\"\"the forward pass of the classifier\r\n",
        "\r\n",
        "        Args:\r\n",
        "            x_in (torch.Tensor): an input data tensor\r\n",
        "                x_in.shape should be (batch, input_dim)\r\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch \r\n",
        "                used fo find the final vector of each sequence\r\n",
        "            apply_softmax (bool): a flag for the softmax activation\r\n",
        "                should be False if used with cross-entropy losses\r\n",
        "        Returns:\r\n",
        "            out (torch.Tensor): out.shape = (batch, num_classes)\r\n",
        "        \"\"\"\r\n",
        "        x_embedded = self.emb(x_in)\r\n",
        "        y_out = self.rnn(x_embedded)\r\n",
        "\r\n",
        "        if x_lengths is not None:\r\n",
        "            y_out = column_gather(y_out, x_lengths)\r\n",
        "        else:\r\n",
        "            y_out = y_out[:, -1, :]\r\n",
        "\r\n",
        "        y_out = F.dropout(y_out, 0.5)\r\n",
        "        y_out = F.relu(self.fc1(y_out))\r\n",
        "        y_out = F.dropout(y_out, 0.5)\r\n",
        "        y_out = self.fc2(y_out)\r\n",
        "\r\n",
        "        if apply_softmax:\r\n",
        "            y_out = F.softmax(y_out, dim=1)\r\n",
        "\r\n",
        "        return y_out\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "*Example 6-5. Retrieving the final output vector in each sequence using column_gather()*\r\n",
        "\r\n",
        "```\r\n",
        "def column_gather(y_out, x_lengths):\r\n",
        "    \"\"\"get a specific vector from each batch data point in `y_out`\r\n",
        "\r\n",
        "    Args:\r\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\r\n",
        "            shape: (batch, sequence, feature)\r\n",
        "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\r\n",
        "            shape: (batch,)\r\n",
        "        \r\n",
        "\r\n",
        "    Returns:\r\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\r\n",
        "            shape: (batch, feature)\r\n",
        "    \"\"\"\r\n",
        "    x_lengths = x_lengths.long().detach().cpu().num() - 1\r\n",
        "\r\n",
        "    out = []\r\n",
        "    for batch_index, column_index in enumerate(x_lengths):\r\n",
        "        out.append(y_out[batch_index, column_index])\r\n",
        "    \r\n",
        "\r\n",
        "    return torch.stack(out)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5BSd6a_yKpf"
      },
      "source": [
        "### The Training Routine and Results\r\n",
        "\r\n",
        "*Example 6-6. Arguments to the RNN-based SurnameClassifer*\r\n",
        "\r\n",
        "```\r\n",
        "args = Namespace(\r\n",
        "    # Data and path information\r\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\r\n",
        "    vectorizer_file=\"vectorizer.json\",\r\n",
        "    model_state_file=\"model.pth\",\r\n",
        "    save_dir=\"model_storage/ch6/surname_classification\",\r\n",
        "    # Model hyperparameters\r\n",
        "    char_embedding_size=100,\r\n",
        "    rnn_hidden_size=64,\r\n",
        "    # Training hyperparameters\r\n",
        "    num_epochs=100,\r\n",
        "    learning_rate=1e-3,\r\n",
        "    batch_size=64,\r\n",
        "    seed=1337,\r\n",
        "    early_stopping_criteria=5,\r\n",
        "    # Runtime options omitted\r\n",
        ")\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXUHSzszyJna"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}